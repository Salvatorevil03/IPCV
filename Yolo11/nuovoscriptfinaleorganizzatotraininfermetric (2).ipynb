{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-31T14:07:29.860575Z",
     "iopub.status.busy": "2025-12-31T14:07:29.860178Z",
     "iopub.status.idle": "2025-12-31T14:07:52.845724Z",
     "shell.execute_reply": "2025-12-31T14:07:52.844926Z",
     "shell.execute_reply.started": "2025-12-31T14:07:29.860544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" PRELIEVO FILE UTILS UFFICIALI \"\"\"\n",
    "# 1. Clona il repository nella cartella di lavoro\n",
    "!git clone https://github.com/DIUx-xView/data_utilities.git\n",
    "!pip install protobuf==3.20.*\n",
    "# 2. (Importante) Aggiungi la cartella al path di Python\n",
    "# Senza questo passaggio, Python non troverà i file .py appena scaricati\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Il percorso sarà /kaggle/working/ + nome_repo\n",
    "repo_path = '/kaggle/working/data_utilities'\n",
    "sys.path.append(repo_path)\n",
    "\n",
    "# 3. Verifica\n",
    "print(\"File scaricati:\", os.listdir(repo_path))\n",
    "\n",
    "# Ora puoi importare le funzioni come se fossero librerie installate\n",
    "# import process_data  <-- Esempio di file presente in quel repo\n",
    "\n",
    "\"\"\" IMPORT DEI FILE PRELEVATI \"\"\"\n",
    "# 1. Definisci dove sono finiti i file .py (se hai clonato la repo xView)\n",
    "# Verifica il nome esatto della cartella che si è creata con il comando !ls\n",
    "cartella_repo = '/kaggle/working/data_utilities' \n",
    "\n",
    "# 2. Aggiungi questa cartella al \"path\" (la lista di dove Python cerca le librerie)\n",
    "if cartella_repo not in sys.path:\n",
    "    sys.path.append(cartella_repo)\n",
    "\n",
    "# 3. ORA puoi importare i file che sono lì dentro\n",
    "# Esempio: nel repo xView c'è spesso un file chiamato 'geojson_utils.py'\n",
    "import aug_util as aug\n",
    "import process_wv\n",
    "import tfr_util\n",
    "import wv_util as wv\n",
    "\n",
    "# Test: Proviamo a vedere se funziona\n",
    "print(\"Import riuscito!\")\n",
    "\n",
    "\"\"\" ALTRE LIBRERIE \"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T14:07:52.847772Z",
     "iopub.status.busy": "2025-12-31T14:07:52.847239Z",
     "iopub.status.idle": "2025-12-31T14:07:52.856314Z",
     "shell.execute_reply": "2025-12-31T14:07:52.855452Z",
     "shell.execute_reply.started": "2025-12-31T14:07:52.847742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" FASE 1: Import e Mappatura Classi\n",
    "Qui carichiamo le librerie necessarie, definiamo i percorsi e creiamo la mappa per convertire gli ID \"strani\" di xView (11, 72...) in ID ordinati per YOLO (0, 1, 2...). \"\"\"\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import wv_util as wv  # La libreria della repo che hai scaricato\n",
    "\n",
    "# --- CONFIGURAZIONE ---\n",
    "GEOJSON_PATH = '/kaggle/input/xview-dataset/train_labels/xView_train.geojson'  # Percorso al tuo geojson\n",
    "IMAGES_DIR = '/kaggle/input/xview-dataset/train_images/train_images'                        # Cartella con le immagini .tif\n",
    "OUTPUT_DIR = 'datasets/xview_yolo'                 # Dove salvare il dataset pronto\n",
    "CHIP_SIZE = 640                                    # Dimensione chip (richiesta da te)\n",
    "\n",
    "def get_id_yolo_mapping(label_file='xview_class_labels.txt'):\n",
    "    \"\"\"Crea un dizionario per convertire ID xView -> ID YOLO (0-59)\"\"\"\n",
    "    labels = {}\n",
    "    with open(label_file) as f:\n",
    "        for line in f:\n",
    "            key, val = line.strip().split(':')\n",
    "            labels[int(key)] = val\n",
    "            \n",
    "    # Ordiniamo gli ID originali per garantire coerenza\n",
    "    sorted_ids = sorted(labels.keys())\n",
    "    \n",
    "    # Creiamo la mappa: {11: 0, 12: 1, ...} e la lista nomi\n",
    "    id_map = {original: new for new, original in enumerate(sorted_ids)}\n",
    "    names = [labels[original] for original in sorted_ids]\n",
    "    \n",
    "    return id_map, names\n",
    "\n",
    "# Eseguiamo subito la mappatura\n",
    "ID_MAP, CLASS_NAMES = get_id_yolo_mapping('/kaggle/working/data_utilities/xview_class_labels.txt')\n",
    "print(f\"Mappatura completata. Classi totali: {len(CLASS_NAMES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T14:07:52.858126Z",
     "iopub.status.busy": "2025-12-31T14:07:52.857553Z",
     "iopub.status.idle": "2025-12-31T14:07:53.158329Z",
     "shell.execute_reply": "2025-12-31T14:07:53.157398Z",
     "shell.execute_reply.started": "2025-12-31T14:07:52.858089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" FASE 2: Creazione Cartelle e Split Train/Val\n",
    "Qui prepariamo la struttura delle directory e dividiamo i nomi dei file .tif per evitare il Data Leakage.\n",
    "-----------------NUOVA AGGIUNTA: Test Set per stima in inferenza------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Crea la struttura di cartelle richiesta da YOLO\"\"\"\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(f\"{OUTPUT_DIR}/{split}/images\", exist_ok=True)\n",
    "        os.makedirs(f\"{OUTPUT_DIR}/{split}/labels\", exist_ok=True)\n",
    "    print(f\"Cartelle create in {OUTPUT_DIR} (train, val, test)\")\n",
    "\n",
    "def split_dataset(val_percent=0.20, test_percent=0.10):\n",
    "    \"\"\"Divide i file .tif in tre liste: train, validation e test\"\"\"\n",
    "    \n",
    "    # Prendi tutti i file .tif nella cartella immagini\n",
    "    all_files = [f for f in os.listdir(IMAGES_DIR) if f.endswith('.tif')]\n",
    "\n",
    "    # Mischia l'ordine (random seed fisso per riproducibilità)\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    total_files = len(all_files)\n",
    "\n",
    "    # Calcola il numero di file per ogni set\n",
    "    n_test = int(total_files * test_percent)\n",
    "    n_val = int(total_files * val_percent)\n",
    "    # Il train prende tutto il resto (per evitare errori di arrotondamento che lasciano fuori 1 file)\n",
    "    n_train = total_files - n_val - n_test\n",
    "\n",
    "    # Slicing delle liste\n",
    "    # 1. I primi n_train file vanno al Training\n",
    "    train_files = all_files[:n_train]\n",
    "    \n",
    "    # 2. I successivi n_val file vanno alla Validation\n",
    "    val_files = all_files[n_train : n_train + n_val]\n",
    "    \n",
    "    # 3. Gli ultimi n_test file vanno al Test\n",
    "    test_files = all_files[n_train + n_val :]\n",
    "\n",
    "    print(f\"Totale immagini originali: {total_files}\")\n",
    "    print(f\"Training (70%):   {len(train_files)} files\")\n",
    "    print(f\"Validation (20%): {len(val_files)} files\")\n",
    "    print(f\"Test (10%):       {len(test_files)} files (Da NON patchare, per SAHI)\")\n",
    "    \n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "# Eseguiamo setup e split con le percentuali richieste (70/20/10)\n",
    "setup_directories()\n",
    "TRAIN_FILES, VAL_FILES, TEST_FILES = split_dataset(val_percent=0.20, test_percent=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T14:07:53.160918Z",
     "iopub.status.busy": "2025-12-31T14:07:53.160617Z",
     "iopub.status.idle": "2025-12-31T14:07:53.172425Z",
     "shell.execute_reply": "2025-12-31T14:07:53.171335Z",
     "shell.execute_reply.started": "2025-12-31T14:07:53.160884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" FASE 3: Funzioni Helper per Conversione\n",
    "Questa è la matematica. Serve una funzione che prenda i box in pixel e li trasformi nel formato 0.5 0.5 0.1 0.2 richiesto da YOLO. \"\"\"\n",
    "\n",
    "def to_yolo_format(box, img_w, img_h):\n",
    "    \"\"\"Converte [xmin, ymin, xmax, ymax] in [x_center, y_center, w, h] normalizzati\"\"\"\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    \n",
    "    # Calcolo centro e dimensioni\n",
    "    dw = 1. / img_w\n",
    "    dh = 1. / img_h\n",
    "    \n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    x_center = xmin + (w / 2.0)\n",
    "    y_center = ymin + (h / 2.0)\n",
    "    \n",
    "    # Normalizzazione (0-1)\n",
    "    x_center *= dw\n",
    "    y_center *= dh\n",
    "    w *= dw\n",
    "    h *= dh\n",
    "    \n",
    "    return x_center, y_center, w, h\n",
    "\n",
    "def save_chip_data(chip_img, boxes, classes, split_type, base_filename, chip_idx):\n",
    "    \"\"\"Salva l'immagine jpg e il file txt corrispondente\"\"\"\n",
    "    \n",
    "    # Nome base univoco per questa chip\n",
    "    filename = f\"{base_filename.replace('.tif', '')}_chip_{chip_idx}\"\n",
    "    \n",
    "    # Percorsi di salvataggio\n",
    "    img_path = f\"{OUTPUT_DIR}/{split_type}/images/{filename}.jpg\"\n",
    "    txt_path = f\"{OUTPUT_DIR}/{split_type}/labels/{filename}.txt\"\n",
    "    \n",
    "    has_objects = False\n",
    "    valid_lines = []\n",
    "\n",
    "    # Processa ogni box nella chip\n",
    "    if boxes is not None:\n",
    "        # Controllo se è un array di zeri (nessun box)\n",
    "        if not (len(boxes) == 1 and (boxes[0] == 0).all()):\n",
    "            for i in range(len(boxes)):\n",
    "                cls_orig = int(classes[i])\n",
    "                \n",
    "                # Se la classe è nella nostra mappa, procedi\n",
    "                if cls_orig in ID_MAP:\n",
    "                    new_cls = ID_MAP[cls_orig]\n",
    "                    xc, yc, w, h = to_yolo_format(boxes[i], CHIP_SIZE, CHIP_SIZE)\n",
    "                    \n",
    "                    # Formatta la riga per il file txt\n",
    "                    line = f\"{new_cls} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\"\n",
    "                    valid_lines.append(line)\n",
    "                    has_objects = True\n",
    "\n",
    "    # SALVATAGGIO:\n",
    "    # Salviamo se ha oggetti OPPURE se vogliamo salvare sfondi vuoti (qui salvo solo se ha oggetti per semplicità)\n",
    "    if has_objects:\n",
    "        # 1. Salva Immagine\n",
    "        img = Image.fromarray(chip_img)\n",
    "        img.save(img_path)\n",
    "        \n",
    "        # 2. Salva Label\n",
    "        with open(txt_path, 'w') as f:\n",
    "            f.write('\\n'.join(valid_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T14:07:53.174243Z",
     "iopub.status.busy": "2025-12-31T14:07:53.173866Z",
     "iopub.status.idle": "2025-12-31T14:14:59.705392Z",
     "shell.execute_reply": "2025-12-31T14:14:59.704627Z",
     "shell.execute_reply.started": "2025-12-31T14:07:53.174214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" FASE 4: Il Motore (Main Loop)\n",
    "Questo è il blocco principale che impiegherà del tempo a girare. Carica l'immagine grande, la chippa e chiama la funzione di salvataggio. \n",
    "------------------NUOVA AGGIUNTA: IL TEST SET NON LO CROPPO--------------------------\n",
    "\"\"\"\n",
    "\n",
    "# 1. Carica tutte le etichette in memoria una volta sola (ci mette un po')\n",
    "print(\"Caricamento GeoJSON globale...\")\n",
    "coords, chips, classes = wv.get_labels(GEOJSON_PATH)\n",
    "\n",
    "def process_files(file_list, split_type):\n",
    "    \"\"\"Itera sulla lista di file, chippa e salva\"\"\"\n",
    "    print(f\"Inizio elaborazione set: {split_type.upper()}...\")\n",
    "    \n",
    "    for fname in tqdm(file_list):\n",
    "        try:\n",
    "            # A. Carica Immagine Originale\n",
    "            img_path = os.path.join(IMAGES_DIR, fname)\n",
    "            arr = wv.get_image(img_path)\n",
    "            \n",
    "            # B. Filtra le label per QUESTA immagine specifica\n",
    "            # Usa la maschera booleana numpy (veloce)\n",
    "            idx = (chips == fname)\n",
    "            im_coords = coords[idx]\n",
    "            im_classes = classes[idx]\n",
    "            \n",
    "            # C. Crea le Chips (640x640)\n",
    "            c_imgs, c_boxes, c_classes = wv.chip_image(arr, im_coords, im_classes, shape=(CHIP_SIZE, CHIP_SIZE))\n",
    "            \n",
    "            # D. Salva ogni chip\n",
    "            for k in range(len(c_imgs)):\n",
    "                save_chip_data(\n",
    "                    chip_img=c_imgs[k],\n",
    "                    boxes=c_boxes.get(k),\n",
    "                    classes=c_classes.get(k),\n",
    "                    split_type=split_type,\n",
    "                    base_filename=fname,\n",
    "                    chip_idx=k\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Errore su {fname}: {e}\")\n",
    "\n",
    "# --- AVVIO PROCESSO ---\n",
    "# Elabora Training\n",
    "process_files(TRAIN_FILES, 'train')\n",
    "\n",
    "# Elabora Validation\n",
    "process_files(VAL_FILES, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T14:14:59.706693Z",
     "iopub.status.busy": "2025-12-31T14:14:59.706431Z",
     "iopub.status.idle": "2025-12-31T14:14:59.734274Z",
     "shell.execute_reply": "2025-12-31T14:14:59.733546Z",
     "shell.execute_reply.started": "2025-12-31T14:14:59.706669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" FASE 5: Creazione YAML\n",
    "Infine, generiamo il file che passerai a YOLO per iniziare l'addestramento. \"\"\"\n",
    "\n",
    "import yaml\n",
    "\n",
    "def create_yaml_file():\n",
    "    yaml_content = {\n",
    "        'path': os.path.abspath(OUTPUT_DIR), # Usa path assoluto per sicurezza\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'nc': len(CLASS_NAMES),\n",
    "        'names': CLASS_NAMES\n",
    "    }\n",
    "    \n",
    "    yaml_path = f\"{OUTPUT_DIR}/custom_xview.yaml\"\n",
    "    \n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_content, f, sort_keys=False)\n",
    "        \n",
    "    print(f\"File YAML creato con successo: {yaml_path}\")\n",
    "    print(\"Controlla che 'names' inizi con:\", CLASS_NAMES[:3])\n",
    "\n",
    "create_yaml_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T14:14:59.735399Z",
     "iopub.status.busy": "2025-12-31T14:14:59.735176Z",
     "iopub.status.idle": "2025-12-31T14:15:04.969300Z",
     "shell.execute_reply": "2025-12-31T14:15:04.968329Z",
     "shell.execute_reply.started": "2025-12-31T14:14:59.735378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T14:15:04.971067Z",
     "iopub.status.busy": "2025-12-31T14:15:04.970738Z",
     "iopub.status.idle": "2025-12-31T22:55:00.945616Z",
     "shell.execute_reply": "2025-12-31T22:55:00.944681Z",
     "shell.execute_reply.started": "2025-12-31T14:15:04.971034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# --- CONFIGURAZIONE ---\n",
    "yaml_path = os.path.abspath('datasets/xview_yolo/custom_xview.yaml')\n",
    "\n",
    "# Parametri di Struttura\n",
    "MODEL_SIZE = 'yolo11m.pt'\n",
    "EPOCHS = 150       \n",
    "BATCH_SIZE = 16   \n",
    "IMG_SIZE = 640    \n",
    "\n",
    "def start_training():\n",
    "    print(f\"Caricamento modello {MODEL_SIZE}...\")\n",
    "    model = YOLO(MODEL_SIZE)\n",
    "\n",
    "    print(f\"Avvio training con iperparametri ottimizzati su: {yaml_path}\")\n",
    "    \n",
    "    # 2. Avvia il training con i parametri da best_hyperparameters.yaml\n",
    "    results = model.train(\n",
    "        # Parametri Dataset e Ambiente\n",
    "        data=yaml_path,\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMG_SIZE,\n",
    "        batch=BATCH_SIZE,\n",
    "        project='runs/train',\n",
    "        name='xview_finetune_v1',\n",
    "        patience=10,\n",
    "        exist_ok=True,\n",
    "        verbose=True,\n",
    "\n",
    "        # Iperparametri di Ottimizzazione (dal tuo YAML)\n",
    "        #optimizer='AdamW',\n",
    "        lr0=0.00606,\n",
    "        lrf=0.00933,\n",
    "        momentum=0.91256,\n",
    "        weight_decay=0.00041,\n",
    "        warmup_epochs=4.35797,\n",
    "        warmup_momentum=0.79154,\n",
    "        box=2.49955,\n",
    "        cls=0.48018,\n",
    "        dfl=1.59442,\n",
    "\n",
    "        # Iperparametri di Augmentation (dal tuo YAML)\n",
    "        hsv_h=0.0184,\n",
    "        hsv_s=0.62745,\n",
    "        hsv_v=0.30035,\n",
    "        degrees=0.0,\n",
    "        translate=0.09503,\n",
    "        scale=0.54519,\n",
    "        shear=0.0,\n",
    "        perspective=0.0,\n",
    "        flipud=0.0,\n",
    "        fliplr=0.49012,\n",
    "        bgr=0.0,\n",
    "        mosaic=1.0,\n",
    "        mixup=0.0,\n",
    "        cutmix=0.0,\n",
    "        copy_paste=0.0,\n",
    "        close_mosaic=7 # Come indicato nello YAML\n",
    "    )\n",
    "    \n",
    "    print(\"Training completato!\")\n",
    "    print(f\"I risultati e i pesi migliori sono salvati in: runs/train/xview_finetune_v1/weights/best.pt\")\n",
    "\n",
    "# --- ESEGUI ---\n",
    "if __name__ == '__main__':\n",
    "    start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T22:55:00.947933Z",
     "iopub.status.busy": "2025-12-31T22:55:00.947406Z",
     "iopub.status.idle": "2025-12-31T22:55:00.968119Z",
     "shell.execute_reply": "2025-12-31T22:55:00.967114Z",
     "shell.execute_reply.started": "2025-12-31T22:55:00.947866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Se la curva della Loss scende e quella della mAP sale, stai andando alla grande. \"\"\"\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Assicurati che il percorso corrisponda al 'name' dato nel training\n",
    "# Esempio: se hai usato name='xview_finetune_v1', il percorso sarà:\n",
    "results_path = 'runs/train/xview_finetune_v1/results.png'\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    print(\"Visualizzazione grafici di training:\")\n",
    "    display(Image(filename=results_path))\n",
    "else:\n",
    "    print(f\"File non trovato in: {results_path}. Controlla se il training è finito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T22:55:00.970833Z",
     "iopub.status.busy": "2025-12-31T22:55:00.970550Z",
     "iopub.status.idle": "2025-12-31T22:55:02.236806Z",
     "shell.execute_reply": "2025-12-31T22:55:02.235975Z",
     "shell.execute_reply.started": "2025-12-31T22:55:00.970806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def predict_and_compare(model_path, val_images_dir, val_labels_dir, class_names=None):\n",
    "    \"\"\"\n",
    "    Pesca una chip casuale dal validation set, fa una predizione e confronta\n",
    "    il risultato con le etichette originali.\n",
    "    \"\"\"\n",
    "    # 1. Carica il modello addestrato\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # 2. Scegli una immagine a caso dalla cartella val\n",
    "    img_files = [f for f in os.listdir(val_images_dir) if f.endswith('.jpg')]\n",
    "    if not img_files:\n",
    "        print(\"Nessuna immagine trovata in val!\")\n",
    "        return\n",
    "        \n",
    "    random_file = random.choice(img_files)\n",
    "    img_path = os.path.join(val_images_dir, random_file)\n",
    "    label_path = os.path.join(val_labels_dir, random_file.replace('.jpg', '.txt'))\n",
    "    \n",
    "    # 3. Esegui la PREDIZIONE (Inference)\n",
    "    # conf=0.25 significa che mostra solo box con sicurezza > 25%\n",
    "    results = model.predict(img_path, conf=0.25, verbose=False)[0]\n",
    "\n",
    "    # --- PLOTTING ---\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Immagine Originale per entrambi i plot\n",
    "    img = Image.open(img_path)\n",
    "    w_img, h_img = img.size\n",
    "    \n",
    "    # A. PLOT SINISTRO: Ground Truth (Verità)\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(f\"Ground Truth (Realtà)\\n{random_file}\")\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                cls, nx, ny, nw, nh = map(float, line.split())\n",
    "                # Denormalizza coordinate (YOLO 0-1 -> Pixel)\n",
    "                w, h = nw * w_img, nh * h_img\n",
    "                x, y = (nx * w_img) - w/2, (ny * h_img) - h/2\n",
    "                \n",
    "                # Disegna Box Verde (Realtà)\n",
    "                rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "                ax[0].add_patch(rect)\n",
    "                \n",
    "                # Testo\n",
    "                txt = class_names[int(cls)] if class_names else str(int(cls))\n",
    "                ax[0].text(x, y-2, txt, color='white', fontsize=9, weight='bold', \n",
    "                           bbox=dict(facecolor='lime', alpha=0.5, edgecolor='none'))\n",
    "    else:\n",
    "        ax[0].text(10, 10, \"Nessun oggetto (Background)\", color='white')\n",
    "\n",
    "    # B. PLOT DESTRO: Predizione Modello\n",
    "    # model.predict restituisce già un oggetto plot facile da usare, ma qui lo facciamo a mano per controllo\n",
    "    ax[1].imshow(img)\n",
    "    ax[1].set_title(f\"Predizione YOLOv11\\nConfidenza > 25%\")\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # Estraiamo i box predetti\n",
    "    for box in results.boxes:\n",
    "        # Coordinate box (xyxy format è il default di results)\n",
    "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "        conf = float(box.conf)\n",
    "        cls = int(box.cls)\n",
    "        \n",
    "        # Disegna Box Rosso (Predizione)\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        rect = patches.Rectangle((x1, y1), w, h, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax[1].add_patch(rect)\n",
    "        \n",
    "        # Testo con confidenza\n",
    "        name = results.names[cls]\n",
    "        label = f\"{name} {conf:.2f}\"\n",
    "        ax[1].text(x1, y1-2, label, color='white', fontsize=9, weight='bold',\n",
    "                   bbox=dict(facecolor='red', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- ESEMPIO DI USO ---\n",
    "# Assicurati di passare il percorso corretto al file best.pt che trovi in runs/train/...\n",
    "predict_and_compare(\n",
    "     model_path='runs/train/xview_finetune_v1/weights/best.pt', \n",
    "     val_images_dir='datasets/xview_yolo/val/images', \n",
    "     val_labels_dir='datasets/xview_yolo/val/labels',\n",
    "     class_names=CLASS_NAMES # La lista nomi che hai creato prima\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENZA CON SAHI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T22:55:02.238270Z",
     "iopub.status.busy": "2025-12-31T22:55:02.237993Z",
     "iopub.status.idle": "2025-12-31T22:55:11.265495Z",
     "shell.execute_reply": "2025-12-31T22:55:11.264544Z",
     "shell.execute_reply.started": "2025-12-31T22:55:02.238243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sahi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T22:55:11.267489Z",
     "iopub.status.busy": "2025-12-31T22:55:11.267011Z",
     "iopub.status.idle": "2025-12-31T22:55:14.928410Z",
     "shell.execute_reply": "2025-12-31T22:55:14.926930Z",
     "shell.execute_reply.started": "2025-12-31T22:55:11.267452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" -------------NUOVA AGGIUNTA: FARO' INFERENZA SUL TEST SET --------------------- \"\"\"\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from sahi.utils.cv import read_image\n",
    "from IPython.display import Image\n",
    "import os, random\n",
    "\n",
    "# 1. Carica il tuo modello YOLO11 personalizzato\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"ultralytics\",\n",
    "    model_path=\"runs/train/xview_finetune_v1/weights/best.pt\",  # any yolov8/yolov9/yolo11/yolo12/rt-detr det model is supported\n",
    "    confidence_threshold=0.25,\n",
    "    device=\"cuda:0\",  # or 'cuda:0' if GPU is available\n",
    ")\n",
    "\n",
    "# VERIFICA PRELIMINARE\n",
    "# Assicuriamoci che la lista TEST_FILES esista in memoria dalla cella precedente\n",
    "if 'TEST_FILES' not in locals() or not TEST_FILES:\n",
    "    raise ValueError(\"Errore: La lista TEST_FILES non è definita! Esegui la cella di 'Split Dataset' prima di questa.\")\n",
    "\n",
    "# 2. Carica l'immagine grande di xView\n",
    "image_folder = \"/kaggle/input/xview-dataset/train_images/train_images\"\n",
    "supported_extensions = ('.tif', '.tiff', '.jpg', '.jpeg', '.png')\n",
    "\n",
    "\"\"\"all_images = [f for f in os.listdir(image_folder) if f.lower().endswith(supported_extensions)]\n",
    "if not all_images:\n",
    "    raise FileNotFoundError(f\"Nessuna immagine trovata in {image_folder}\")\n",
    "\n",
    "random_image_name = random.choice(all_images)\n",
    "random_image_path = os.path.join(image_folder, random_image_name)\"\"\"\n",
    "\n",
    "# Qui sta la magia: invece di os.listdir, usiamo la lista filtrata\n",
    "random_image_name = random.choice(TEST_FILES)\n",
    "random_image_path = os.path.join(image_folder, random_image_name)\n",
    "\n",
    "print(f\"Immagine selezionata per SAHI: {random_image_name}\")\n",
    "\n",
    "#image = read_image(\"large_xview_image.tif\")\n",
    "\n",
    "# 3. Esegui l'inferenza a fette (Slicing)\n",
    "result = get_sliced_prediction(\n",
    "    random_image_path,\n",
    "    detection_model,\n",
    "    slice_height=640,        # Dimensione della fetta (coerente con il tuo training)\n",
    "    slice_width=640,\n",
    "    overlap_height_ratio=0.2, # Sovrapposizione per evitare di tagliare oggetti a metà\n",
    "    overlap_width_ratio=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Visualizza o salva i risultati\n",
    "result.export_visuals(\n",
    "    export_dir=\"results_sahi/\" + str(random_image_name),\n",
    "    file_name=\"prediction_visual\",\n",
    "    rect_th=1,       # Spessore del rettangolo (bounding box): 1 pixel\n",
    "    text_size=0.3,   # Dimensione del testo: prova 0.3 o 0.4\n",
    "    #text_th=1        # Spessore del testo: 1 pixel\n",
    ")\n",
    "# Separa il nome dall'estensione (es. '2279.tif' -> '2279')\n",
    "image_name_only = os.path.splitext(random_image_name)[0]\n",
    "Image('/kaggle/working/results_sahi/'+str(image_name_only)+'.tif/prediction_visual.png',width=2048) #/kaggle/working/results_sahi/310.tif/prediction_visual.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T22:55:14.932623Z",
     "iopub.status.busy": "2025-12-31T22:55:14.930319Z",
     "iopub.status.idle": "2025-12-31T22:55:14.949297Z",
     "shell.execute_reply": "2025-12-31T22:55:14.947390Z",
     "shell.execute_reply.started": "2025-12-31T22:55:14.932563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "random_image_name = random.choice(all_images)\n",
    "random_image_path = os.path.join(image_folder, random_image_name)\n",
    "print(f\"Immagine selezionata per SAHI: {random_image_name}\")\n",
    "\n",
    "#image = read_image(\"large_xview_image.tif\")\n",
    "\n",
    "# 3. Esegui l'inferenza a fette (Slicing)\n",
    "result = get_sliced_prediction(\n",
    "    random_image_path,\n",
    "    detection_model,\n",
    "    slice_height=640,        # Dimensione della fetta (coerente con il tuo training)\n",
    "    slice_width=640,\n",
    "    overlap_height_ratio=0.2, # Sovrapposizione per evitare di tagliare oggetti a metà\n",
    "    overlap_width_ratio=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Visualizza o salva i risultati\n",
    "result.export_visuals(\n",
    "    export_dir=\"results_sahi/\" + str(random_image_name),\n",
    "    file_name=\"prediction_visual\",\n",
    "    rect_th=1,       # Spessore del rettangolo (bounding box): 1 pixel\n",
    "    text_size=0.3,   # Dimensione del testo: prova 0.3 o 0.4\n",
    "    #text_th=1        # Spessore del testo: 1 pixel\n",
    ")\n",
    "# Separa il nome dall'estensione (es. '2279.tif' -> '2279')\n",
    "image_name_only = os.path.splitext(random_image_name)[0]\n",
    "Image('/kaggle/working/results_sahi/'+str(image_name_only)+'.tif/prediction_visual.png',width=2048) #/kaggle/working/results_sahi/310.tif/prediction_visual.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCOLO DELLE METRICHE IN INFERENZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:01:07.254660Z",
     "iopub.status.busy": "2026-01-01T00:01:07.254296Z",
     "iopub.status.idle": "2026-01-01T00:05:51.048045Z",
     "shell.execute_reply": "2026-01-01T00:05:51.047030Z",
     "shell.execute_reply.started": "2026-01-01T00:01:07.254629Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" FASE 6: Calcolo Metriche su Test Set (mAP, Precision, Recall) con SAHI\n",
    "Questo script converte Ground Truth e Predizioni in formato COCO e calcola le metriche.\n",
    "\"\"\"\n",
    "import json\n",
    "import time\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "# Assicuriamoci di avere le variabili globali caricate (coords, chips, classes, ID_MAP, TEST_FILES)\n",
    "if 'TEST_FILES' not in locals() or 'coords' not in locals():\n",
    "    raise ValueError(\"Mancano le variabili del dataset. Esegui le celle precedenti!\")\n",
    "\n",
    "def create_coco_gt(test_files, image_folder):\n",
    "    \"\"\"Crea il dizionario COCO per il Ground Truth delle sole immagini di Test (Corretto con 'info')\"\"\"\n",
    "    \n",
    "    # --- CORREZIONE QUI: Aggiunta chiave 'info' e 'licenses' richieste da pycocotools ---\n",
    "    coco_gt = {\n",
    "        \"info\": {\n",
    "            \"description\": \"xView Custom Test Set\",\n",
    "            \"url\": \"\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2024,\n",
    "            \"contributor\": \"\",\n",
    "            \"date_created\": \"\"\n",
    "        },\n",
    "        \"licenses\": [],\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Crea Categorie\n",
    "    unique_yolo_ids = sorted(list(set(ID_MAP.values())))\n",
    "    for y_id in unique_yolo_ids:\n",
    "        coco_gt[\"categories\"].append({\"id\": int(y_id), \"name\": str(y_id), \"supercategory\": \"object\"})\n",
    "\n",
    "    ann_id_counter = 1\n",
    "    \n",
    "    print(\"Generazione Ground Truth COCO per il Test Set (con metadati corretti)...\")\n",
    "    \n",
    "    for file_idx, fname in enumerate(tqdm(test_files)):\n",
    "        image_id = file_idx  \n",
    "        \n",
    "        # Leggiamo dimensioni reali\n",
    "        img_full_path = os.path.join(image_folder, fname)\n",
    "        try:\n",
    "            with Image.open(img_full_path) as img:\n",
    "                width, height = img.size\n",
    "        except Exception as e:\n",
    "            # print(f\"Errore lettura {fname}: {e}. Uso default.\") # Commentato per pulizia output\n",
    "            width, height = 3000, 3000 \n",
    "\n",
    "        coco_gt[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": fname,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "        \n",
    "        # Aggiungi Annotazioni\n",
    "        idx = (chips == fname)\n",
    "        im_coords = coords[idx]\n",
    "        im_classes = classes[idx]\n",
    "        \n",
    "        for box, cls_orig in zip(im_coords, im_classes):\n",
    "            if int(cls_orig) in ID_MAP:\n",
    "                cls_yolo = ID_MAP[int(cls_orig)]\n",
    "                \n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                w = xmax - xmin\n",
    "                h = ymax - ymin\n",
    "                \n",
    "                coco_gt[\"annotations\"].append({\n",
    "                    \"id\": ann_id_counter,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": int(cls_yolo),\n",
    "                    \"bbox\": [float(xmin), float(ymin), float(w), float(h)],\n",
    "                    \"area\": float(w * h),\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                ann_id_counter += 1\n",
    "                \n",
    "    return coco_gt\n",
    "   \n",
    "\n",
    "# --- 2. ESECUZIONE INFERENZA SU TUTTO IL TEST SET ---\n",
    "def run_sahi_inference_and_export(test_files, image_folder, model):\n",
    "    \"\"\"Esegue SAHI su tutte le immagini di test e raccoglie i risultati in formato COCO\"\"\"\n",
    "    coco_results = []\n",
    "    \n",
    "    print(\"\\nAvvio Inferenza SAHI su Test Set...\")\n",
    "    for file_idx, fname in enumerate(tqdm(test_files)):\n",
    "        image_id = file_idx # Deve corrispondere all'ID nel GT\n",
    "        img_path = os.path.join(image_folder, fname)\n",
    "        \n",
    "        # Inferenza SAHI\n",
    "        result = get_sliced_prediction(\n",
    "            img_path,\n",
    "            model,\n",
    "            slice_height=640,\n",
    "            slice_width=640,\n",
    "            overlap_height_ratio=0.2,\n",
    "            overlap_width_ratio=0.2,\n",
    "            verbose=0 # Silenzioso\n",
    "        )\n",
    "        \n",
    "        # Estrai predizioni\n",
    "        for pred in result.object_prediction_list:\n",
    "            # bbox in SAHI è [xmin, ymin, xmax, ymax]\n",
    "            x_min, y_min, x_max, y_max = pred.bbox.to_xyxy()\n",
    "            w = x_max - x_min\n",
    "            h = y_max - y_min\n",
    "            \n",
    "            score = pred.score.value\n",
    "            category_id = pred.category.id # Questo è l'ID YOLO (0..N) restituito dal modello\n",
    "            \n",
    "            coco_results.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": int(category_id),\n",
    "                \"bbox\": [float(x_min), float(y_min), float(w), float(h)],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "            \n",
    "    return coco_results\n",
    "\n",
    "# --- MAIN FLOW ---\n",
    "\n",
    "# 1. Prepara i dati GT\n",
    "gt_json = create_coco_gt(TEST_FILES, \"/kaggle/input/xview-dataset/train_images/train_images\")\n",
    "\n",
    "# Salva GT su disco (necessario per COCO API)\n",
    "with open('test_gt.json', 'w') as f:\n",
    "    json.dump(gt_json, f)\n",
    "\n",
    "# 2. Ottieni Predizioni (Usa il 'detection_model' che hai caricato nella cella precedente)\n",
    "# Attenzione: Questo loop può impiegare TEMPO a seconda di quante immagini di test hai.\n",
    "# Per test veloce, puoi ridurre TEST_FILES[:10]\n",
    "predictions = run_sahi_inference_and_export(TEST_FILES, \"/kaggle/input/xview-dataset/train_images/train_images\", detection_model)\n",
    "\n",
    "# Salva Predizioni su disco\n",
    "with open('test_pred.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "# --- 3. CALCOLO METRICHE ---\n",
    "print(\"\\n--- RISULTATI VALUTAZIONE ---\")\n",
    "\n",
    "# Inizializza COCO\n",
    "cocoGt = COCO('test_gt.json')\n",
    "cocoDt = cocoGt.loadRes('test_pred.json')\n",
    "\n",
    "# Esegui COCOeval\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()\n",
    "\n",
    "print(\"\\nLegenda:\")\n",
    "print(\"AP @ .50:95 = mAP (Metrica primaria)\")\n",
    "print(\"AP @ .50    = mAP50 (Spesso usata in YOLO)\")\n",
    "print(\"AR @ 100    = Recall (per un massimo di 100 detection per immagine)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per ricordare gli split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T23:58:58.541239Z",
     "iopub.status.busy": "2025-12-31T23:58:58.540540Z",
     "iopub.status.idle": "2025-12-31T23:58:58.546041Z",
     "shell.execute_reply": "2025-12-31T23:58:58.545233Z",
     "shell.execute_reply.started": "2025-12-31T23:58:58.541180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(TEST_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T23:59:01.684413Z",
     "iopub.status.busy": "2025-12-31T23:59:01.684072Z",
     "iopub.status.idle": "2025-12-31T23:59:01.689143Z",
     "shell.execute_reply": "2025-12-31T23:59:01.688113Z",
     "shell.execute_reply.started": "2025-12-31T23:59:01.684383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(VAL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T22:55:31.174455Z",
     "iopub.status.busy": "2025-12-31T22:55:31.173640Z",
     "iopub.status.idle": "2025-12-31T23:54:15.339255Z",
     "shell.execute_reply": "2025-12-31T23:54:15.332950Z",
     "shell.execute_reply.started": "2025-12-31T22:55:31.174420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "i=0\n",
    "print('Notebook finito, itero nella tua attesa master')\n",
    "while(True):\n",
    "    print('Numero ore attese: ',i)\n",
    "    i=i+1\n",
    "    time.sleep(3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altro tentativo di valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" QUESTA è UNA VALUTAZIONE SEMPRE SUL DATASET TAGLIATO \"\"\"\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(\"/kaggle/working/runs/train/xview_finetune_v1/weights/best.pt\")\n",
    "\n",
    "# Run the evaluation\n",
    "results = model.val(data=\"/kaggle/working/datasets/xview_yolo/custom_xview.yaml\")\n",
    "\n",
    "# Print specific metrics\n",
    "print(\"Class indices with average precision:\", results.ap_class_index)\n",
    "print(\"Average precision for all classes:\", results.box.all_ap)\n",
    "print(\"Average precision:\", results.box.ap)\n",
    "print(\"Average precision at IoU=0.50:\", results.box.ap50)\n",
    "print(\"Class indices for average precision:\", results.box.ap_class_index)\n",
    "print(\"Class-specific results:\", results.box.class_result)\n",
    "print(\"F1 score:\", results.box.f1)\n",
    "print(\"F1 score curve:\", results.box.f1_curve)\n",
    "print(\"Overall fitness score:\", results.box.fitness)\n",
    "print(\"Mean average precision:\", results.box.map)\n",
    "print(\"Mean average precision at IoU=0.50:\", results.box.map50)\n",
    "print(\"Mean average precision at IoU=0.75:\", results.box.map75)\n",
    "print(\"Mean average precision for different IoU thresholds:\", results.box.maps)\n",
    "print(\"Mean results for different metrics:\", results.box.mean_results)\n",
    "print(\"Mean precision:\", results.box.mp)\n",
    "print(\"Mean recall:\", results.box.mr)\n",
    "print(\"Precision:\", results.box.p)\n",
    "print(\"Precision curve:\", results.box.p_curve)\n",
    "print(\"Precision values:\", results.box.prec_values)\n",
    "print(\"Specific precision metrics:\", results.box.px)\n",
    "print(\"Recall:\", results.box.r)\n",
    "print(\"Recall curve:\", results.box.r_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:17:12.007103Z",
     "iopub.status.busy": "2026-01-01T00:17:12.006687Z",
     "iopub.status.idle": "2026-01-01T00:17:12.043093Z",
     "shell.execute_reply": "2026-01-01T00:17:12.042205Z",
     "shell.execute_reply.started": "2026-01-01T00:17:12.007069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" REPORT PIù LEGGIBILE \"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Metriche Globali\n",
    "print(f\"{'METRICA':<20} | {'VALORE':<10}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'mAP @ 50-95':<20} | {results.box.map:.4f}\")\n",
    "print(f\"{'mAP @ 50':<20} | {results.box.map50:.4f}\")\n",
    "print(f\"{'Precision (Media)':<20} | {results.box.mp:.4f}\")\n",
    "print(f\"{'Recall (Media)':<20} | {results.box.mr:.4f}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# 2. Analisi per Classe (Top & Flop)\n",
    "# Creiamo un DataFrame per leggere meglio i dati\n",
    "# Nota: Assicurati che 'model.names' contenga i nomi delle classi\n",
    "class_names = results.names  # O la tua lista CLASS_NAMES\n",
    "maps_per_class = results.box.maps  # mAP 50-95 per ogni classe\n",
    "\n",
    "df_classes = pd.DataFrame({\n",
    "    'Classe': class_names.values(), # o solo class_names se è una lista\n",
    "    'mAP 50-95': maps_per_class\n",
    "})\n",
    "\n",
    "# Ordiniamo per vedere le migliori e le peggiori\n",
    "df_sorted = df_classes.sort_values(by='mAP 50-95', ascending=False)\n",
    "\n",
    "print(\"\\n--- MIGLIORI 5 CLASSI ---\")\n",
    "print(df_sorted.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\n--- PEGGIORI 5 CLASSI ---\")\n",
    "print(df_sorted.tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:15:39.336148Z",
     "iopub.status.busy": "2026-01-01T00:15:39.335387Z",
     "iopub.status.idle": "2026-01-01T00:15:39.404629Z",
     "shell.execute_reply": "2026-01-01T00:15:39.403722Z",
     "shell.execute_reply.started": "2026-01-01T00:15:39.336114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print specific metrics\n",
    "print(\"Class indices with average precision:\", results.ap_class_index)\n",
    "print(\"Average precision for all classes:\", results.box.all_ap)\n",
    "print(\"Average precision:\", results.box.ap)\n",
    "print(\"Average precision at IoU=0.50:\", results.box.ap50)\n",
    "print(\"Class indices for average precision:\", results.box.ap_class_index)\n",
    "print(\"Class-specific results:\", results.box.class_result)\n",
    "print(\"F1 score:\", results.box.f1)\n",
    "print(\"F1 score curve:\", results.box.f1_curve)\n",
    "print(\"Overall fitness score:\", results.box.fitness)\n",
    "print(\"Mean average precision:\", results.box.map)\n",
    "print(\"Mean average precision at IoU=0.50:\", results.box.map50)\n",
    "print(\"Mean average precision at IoU=0.75:\", results.box.map75)\n",
    "print(\"Mean average precision for different IoU thresholds:\", results.box.maps)\n",
    "print(\"Mean results for different metrics:\", results.box.mean_results)\n",
    "print(\"Mean precision:\", results.box.mp)\n",
    "print(\"Mean recall:\", results.box.mr)\n",
    "print(\"Precision:\", results.box.p)\n",
    "print(\"Precision curve:\", results.box.p_curve)\n",
    "print(\"Precision values:\", results.box.prec_values)\n",
    "print(\"Specific precision metrics:\", results.box.px)\n",
    "print(\"Recall:\", results.box.r)\n",
    "print(\"Recall curve:\", results.box.r_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentativo al fly di validazione sul test set grosso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:21:25.719782Z",
     "iopub.status.busy": "2026-01-01T00:21:25.719446Z",
     "iopub.status.idle": "2026-01-01T00:26:17.925021Z",
     "shell.execute_reply": "2026-01-01T00:26:17.924212Z",
     "shell.execute_reply.started": "2026-01-01T00:21:25.719750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" FASE 6: Calcolo Metriche Ufficiali su TEST SET (Immagini Intere)\n",
    "Non serve YAML: usiamo direttamente i percorsi dei file e la lista TEST_FILES.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sahi.predict import get_sliced_prediction\n",
    "\n",
    "# --- 1. CONFIGURAZIONE PERCORSI ---\n",
    "# Percorso delle immagini originali (preso dal tuo screenshot)\n",
    "# NOTA: In xView su Kaggle spesso il path è doppiato 'train_images/train_images'\n",
    "ORIGINAL_IMAGES_DIR = \"/kaggle/input/xview-dataset/train_images/train_images\"\n",
    "\n",
    "# Verifica che la cartella esista\n",
    "if not os.path.exists(ORIGINAL_IMAGES_DIR):\n",
    "    raise FileNotFoundError(f\"Non trovo la cartella: {ORIGINAL_IMAGES_DIR}\")\n",
    "\n",
    "# Verifica variabili necessarie\n",
    "if 'TEST_FILES' not in locals() or 'coords' not in locals():\n",
    "    raise ValueError(\"ERRORE: Mancano le variabili 'TEST_FILES' o 'coords'. Esegui le celle di Split e Caricamento dati!\")\n",
    "\n",
    "print(f\"Userò {len(TEST_FILES)} immagini per il test (dal 10% di split creato prima).\")\n",
    "\n",
    "# --- 2. FUNZIONE GENERAZIONE GROUND TRUTH (COCO) ---\n",
    "def create_coco_gt_from_memory(test_files, image_folder):\n",
    "    \"\"\"\n",
    "    Crea il JSON del Ground Truth convertendo al volo le annotazioni\n",
    "    presenti nelle variabili globali 'chips', 'coords', 'classes'.\n",
    "    \"\"\"\n",
    "    coco_gt = {\n",
    "        \"info\": {\"description\": \"xView Test Set\"}, # Necessario per pycocotools\n",
    "        \"licenses\": [],\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "    \n",
    "    # Crea Categorie\n",
    "    unique_ids = sorted(list(set(ID_MAP.values())))\n",
    "    for y_id in unique_ids:\n",
    "        # Usa il nome della classe se disponibile, altrimenti l'ID\n",
    "        c_name = CLASS_NAMES[y_id] if 'CLASS_NAMES' in locals() else str(y_id)\n",
    "        coco_gt[\"categories\"].append({\"id\": int(y_id), \"name\": c_name, \"supercategory\": \"object\"})\n",
    "\n",
    "    ann_id = 1\n",
    "    \n",
    "    print(\"Generazione file Ground Truth (GT)...\")\n",
    "    for idx_img, fname in enumerate(tqdm(test_files)):\n",
    "        full_path = os.path.join(image_folder, fname)\n",
    "        \n",
    "        # 1. Recupera dimensioni reali immagine (importante per mAP corretta)\n",
    "        try:\n",
    "            with Image.open(full_path) as img:\n",
    "                width, height = img.size\n",
    "        except:\n",
    "            width, height = 3000, 3000 # Fallback\n",
    "            \n",
    "        coco_gt[\"images\"].append({\n",
    "            \"id\": idx_img, \n",
    "            \"file_name\": fname, \n",
    "            \"width\": width, \n",
    "            \"height\": height\n",
    "        })\n",
    "        \n",
    "        # 2. Recupera le label dalle variabili globali di xView\n",
    "        # Trova gli indici dove il nome file corrisponde\n",
    "        mask = (chips == fname)\n",
    "        im_boxes = coords[mask]   # [xmin, ymin, xmax, ymax]\n",
    "        im_classes = classes[mask]\n",
    "        \n",
    "        for box, cls_orig in zip(im_boxes, im_classes):\n",
    "            if int(cls_orig) in ID_MAP:\n",
    "                cls_yolo = ID_MAP[int(cls_orig)]\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                w = xmax - xmin\n",
    "                h = ymax - ymin\n",
    "                \n",
    "                coco_gt[\"annotations\"].append({\n",
    "                    \"id\": ann_id,\n",
    "                    \"image_id\": idx_img,\n",
    "                    \"category_id\": int(cls_yolo),\n",
    "                    \"bbox\": [float(xmin), float(ymin), float(w), float(h)],\n",
    "                    \"area\": float(w*h),\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                ann_id += 1\n",
    "                \n",
    "    return coco_gt\n",
    "\n",
    "# --- 3. FUNZIONE INFERENZA (SAHI) ---\n",
    "def run_sahi_evaluation(test_files, image_folder, model):\n",
    "    coco_preds = []\n",
    "    \n",
    "    print(\"\\nAvvio Inferenza SAHI...\")\n",
    "    for idx_img, fname in enumerate(tqdm(test_files)):\n",
    "        full_path = os.path.join(image_folder, fname)\n",
    "        \n",
    "        # Inferenza SAHI\n",
    "        # slice_height/width: usa 640 (come il training)\n",
    "        # overlap: 0.2 standard, puoi provare 0.5 per migliorare i risultati\n",
    "        result = get_sliced_prediction(\n",
    "            full_path,\n",
    "            model,\n",
    "            slice_height=640,\n",
    "            slice_width=640,\n",
    "            overlap_height_ratio=0.2,\n",
    "            overlap_width_ratio=0.2,\n",
    "            verbose=0,\n",
    "            postprocess_match_metric=\"IOS\" # Gestione box sovrapposti nel merge\n",
    "        )\n",
    "        \n",
    "        # Estrai predizioni\n",
    "        for pred in result.object_prediction_list:\n",
    "            x, y, x2, y2 = pred.bbox.to_xyxy()\n",
    "            w = x2 - x\n",
    "            h = y2 - y\n",
    "            \n",
    "            coco_preds.append({\n",
    "                \"image_id\": idx_img,\n",
    "                \"category_id\": int(pred.category.id),\n",
    "                \"bbox\": [float(x), float(y), float(w), float(h)],\n",
    "                \"score\": float(pred.score.value)\n",
    "            })\n",
    "            \n",
    "    return coco_preds\n",
    "\n",
    "# ==========================================\n",
    "#         ESECUZIONE PRINCIPALE\n",
    "# ==========================================\n",
    "\n",
    "# A. Genera e Salva Ground Truth\n",
    "gt_data = create_coco_gt_from_memory(TEST_FILES, ORIGINAL_IMAGES_DIR)\n",
    "with open(\"test_gt.json\", \"w\") as f:\n",
    "    json.dump(gt_data, f)\n",
    "\n",
    "# B. Genera e Salva Predizioni (SAHI)\n",
    "# Assicurati che 'detection_model' sia caricato (cella precedente)\n",
    "pred_data = run_sahi_evaluation(TEST_FILES, ORIGINAL_IMAGES_DIR, detection_model)\n",
    "with open(\"test_pred.json\", \"w\") as f:\n",
    "    json.dump(pred_data, f)\n",
    "\n",
    "# C. Calcola Metriche\n",
    "print(\"\\n--- CALCOLO METRICHE ---\")\n",
    "cocoGt = COCO(\"test_gt.json\")\n",
    "cocoDt = cocoGt.loadRes(\"test_pred.json\")\n",
    "\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, \"bbox\")\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1561333,
     "sourceId": 2571636,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
