{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2571636,"sourceType":"datasetVersion","datasetId":1561333}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTS # ","metadata":{}},{"cell_type":"code","source":"import json\nimport gc\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as T\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\nimport numpy as np\nimport cv2\nimport json\nimport os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm \nfrom pathlib import Path\nimport torch\nimport cv2\nimport numpy as np\nimport json\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport random\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport shutil\nfrom torchvision.models.detection.rpn import AnchorGenerator,RPNHead\nfrom tqdm.notebook import tqdm\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:53:34.425628Z","iopub.execute_input":"2026-01-04T22:53:34.426262Z","iopub.status.idle":"2026-01-04T22:53:51.358528Z","shell.execute_reply.started":"2026-01-04T22:53:34.426234Z","shell.execute_reply":"2026-01-04T22:53:51.357896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Studio di xView ","metadata":{}},{"cell_type":"code","source":"geojson_path = '/kaggle/input/xview-dataset/train_labels/xView_train.geojson' \nwith open(geojson_path) as f:\n    data = json.load(f)\n\nclass_ids = [feature['properties']['type_id'] for feature in data['features']]\n\ndf_classes = pd.DataFrame(class_ids, columns=['type_id'])\n\ncounts = df_classes['type_id'].value_counts().sort_index()\n\nnum_classes = df_classes['type_id'].nunique()\n\nprint(f\"Totale classi: {num_classes}\")\nprint(f\"Totale oggetti annotati: {len(df_classes)}\")\nprint(\"\\n--- Conteggio per Class ID ---\")\nprint(df_classes['type_id'].value_counts().sort_index(ascending=False).head(60)) #altrimenti me ne lista solamente 10 (5 top e 5 bottom)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:50:45.981102Z","iopub.execute_input":"2026-01-04T12:50:45.981515Z","iopub.status.idle":"2026-01-04T12:50:55.025118Z","shell.execute_reply.started":"2026-01-04T12:50:45.981490Z","shell.execute_reply":"2026-01-04T12:50:55.024523Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tiling di xView # ","metadata":{}},{"cell_type":"markdown","source":"# Funzione per prelevare immagini xview #\n\ntrasformo il geojson in un json normale (per ogni img)","metadata":{}},{"cell_type":"code","source":"def load_xview_data(geojson_path): # prendo geojson e creo un json/dizionario organizzato per id dell'img\n\n    print(\"Caricamento GeoJSON... (può richiedere un po')\")\n    with open(geojson_path) as f:\n        data = json.load(f)\n    \n    coords_by_img = {}\n    \n    for feature in tqdm(data['features']):\n        props = feature['properties']\n        img_name = props['image_id']\n        bounds = [int(float(x)) for x in props['bounds_imcoords'].split(',')]\n        type_id = props['type_id']\n        \n        if img_name not in coords_by_img:\n            coords_by_img[img_name] = [] # dizionario di liste (key: img_name -> value: lista di annotazioni)\n            \n        coords_by_img[img_name].append({\n            'bounds': bounds,\n            'type_id': type_id\n        })\n        \n    return coords_by_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:54:05.550729Z","iopub.execute_input":"2026-01-04T22:54:05.551668Z","iopub.status.idle":"2026-01-04T22:54:05.557088Z","shell.execute_reply.started":"2026-01-04T22:54:05.551635Z","shell.execute_reply":"2026-01-04T22:54:05.556275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Funzione di Tiling #\nUsata per entrambi i dataset (train,val)","metadata":{}},{"cell_type":"code","source":"def tile_dataset(image_list, output_dir, output_json_path, TILE_SIZE, STRIDE, coords_by_img):\n    subset_data = []\n    \n    # --- PARAMETRI DI FILTRAGGIO ---\n    MIN_AREA = 16 * 16  # 256 pixel quadrati (es. un box 16x16)\n    MAX_TRUNCATION = 0.50 # Scarta se più del 50% dell'oggetto è fuori dal tile\n    MIN_DIM = 4 # Lato minimo in pixel\n    # -------------------------------\n\n    for img_name in tqdm(image_list):\n        img_path = os.path.join(INPUT_IMAGES_DIR, img_name)\n        img = cv2.imread(img_path)\n        \n        if img is None: \n            continue\n                \n        h, w, _ = img.shape\n        original_boxes = coords_by_img[img_name]\n\n        # Tiling Loop\n        for y in range(0, h, STRIDE):\n            for x in range(0, w, STRIDE):\n                x2 = min(x + TILE_SIZE, w)\n                y2 = min(y + TILE_SIZE, h)\n                x1 = x2 - TILE_SIZE\n                y1 = y2 - TILE_SIZE\n                if x1 < 0: x1 = 0 \n                if y1 < 0: y1 = 0\n                \n                # Estrai la tile\n                tile_img = img[y1:y2, x1:x2]\n                \n                valid_objects = []\n                for obj in original_boxes:\n                    bx1, by1, bx2, by2 = obj['bounds']\n                    \n                    # Calcolo Area Originale (per il troncamento)\n                    original_w = bx2 - bx1\n                    original_h = by2 - by1\n                    original_area = original_w * original_h\n                    \n                    # Intersezione (Box nel Tile)\n                    ix1 = max(x1, bx1)\n                    iy1 = max(y1, by1)\n                    ix2 = min(x2, bx2) \n                    iy2 = min(y2, by2)\n                    \n                    if ix1 < ix2 and iy1 < iy2:\n                        # Coordinate locali nella tile\n                        nx1 = ix1 - x1\n                        ny1 = iy1 - y1\n                        nx2 = ix2 - x1\n                        ny2 = iy2 - y1\n                        \n                        new_w = nx2 - nx1\n                        new_h = ny2 - ny1\n                        new_area = new_w * new_h\n                        \n                        # --- FILTRI AVANZATI ---\n                        \n                        # 1. Filtro Dimensione Minima (Pixel)\n                        if new_w < MIN_DIM or new_h < MIN_DIM:\n                            continue # Troppo sottile/piccolo\n                            \n                        # 2. Filtro Area Minima\n                        if new_area < MIN_AREA:\n                            continue # Oggetto troppo piccolo per essere riconosciuto\n                            \n                        # 3. Filtro Troncamento (Cruciale!)\n                        # Quanta parte dell'oggetto originale è finita in questo tile?\n                        # Se ho solo il 30% dell'oggetto (truncation > 0.7), lo butto.\n                        # (1 - ratio) ci dà la frazione mancante.\n                        visible_ratio = new_area / original_area\n                        truncation = 1.0 - visible_ratio\n                        \n                        if truncation > MAX_TRUNCATION:\n                            continue # Oggetto troppo tagliato\n                        \n                        # Se passa i filtri, lo salviamo\n                        valid_objects.append({\n                            'bounds': [nx1, ny1, nx2, ny2],\n                            'type_id': obj['type_id']\n                        })\n                \n                # Salva solo se la tile ha oggetti validi sopravvissuti ai filtri\n                if len(valid_objects) > 0:\n                    tile_filename = f\"{Path(img_name).stem}_tile_{x}_{y}.jpg\"\n                    save_path = os.path.join(output_dir, tile_filename)\n                    cv2.imwrite(save_path, tile_img)\n                    subset_data.append({\n                        'img_name': tile_filename,\n                        'objects': valid_objects\n                    })\n\n    print(f\"Creati {len(subset_data)} tiles validi.\")\n                    \n    with open(output_json_path, 'w') as f:\n        json.dump(subset_data, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:54:10.886136Z","iopub.execute_input":"2026-01-04T22:54:10.886640Z","iopub.status.idle":"2026-01-04T22:54:10.897356Z","shell.execute_reply.started":"2026-01-04T22:54:10.886610Z","shell.execute_reply":"2026-01-04T22:54:10.896556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CONFIGURAZIONE ---\nINPUT_IMAGES_DIR = '/kaggle/input/xview-dataset/train_images/train_images' \nINPUT_GEOJSON = '/kaggle/input/xview-dataset/train_labels/xView_train.geojson'\n\n# Cartelle di output\nTRAIN_DIR = '/kaggle/working/train_tiles'\nVAL_DIR = '/kaggle/working/val_tiles'\nTRAIN_JSON = '/kaggle/working/train_annotations.json'\nVAL_JSON = '/kaggle/working/val_annotations.json'\nTEST_JSON = '/kaggle/working/test_annotations.json'\nTEST_GT_JSON = '/kaggle/working/test_gt_annotations.json'\n\n# Pulizia cartelle\nif os.path.exists(TRAIN_DIR): shutil.rmtree(TRAIN_DIR)\nif os.path.exists(VAL_DIR): shutil.rmtree(VAL_DIR)\nos.makedirs(TRAIN_DIR)\nos.makedirs(VAL_DIR)\n\nTILE_SIZE = 512\nSTRIDE = 400\n\ndef create_datasets(num_train, num_val, num_test):\n    \n    # 1. Carica TUTTE le immagini disponibili\n    print(\"Caricamento lista immagini...\")\n    coords_by_img = load_xview_data(INPUT_GEOJSON) \n    \n    # Questa è l'urna con tutte le 847 immagini\n    available_images = list(coords_by_img.keys()) \n    total_available = len(available_images)\n    \n    print(f\"Totale immagini nell'urna: {total_available}\")\n    \n    # Controllo sicurezza\n    if (num_train + num_val + num_test) > total_available:\n        print(\"ERRORE: Hai chiesto più immagini di quelle disponibili!\")\n        return\n\n    # Liste vuote da riempire\n    train_imgs = []\n    val_imgs = []\n    test_imgs = []\n\n    # --- ESTRAZIONE TRAINING SET ---\n    print(f\"\\n--- Estrazione casuale di {num_train} immagini per il TRAIN ---\")\n    random.seed(42) # Seed per riproducibilità (rimuovilo se vuoi casualità pura ogni volta)\n    \n    for i in range(num_train):\n        # 1. Genera un indice casuale tra 0 e l'ultimo indice disponibile rimasto\n        # Nota: len(available_images) diminuisce a ogni giro!\n        random_index = random.randint(0, len(available_images) - 1)\n        \n        # 2. 'pop' estrae l'elemento a quell'indice e lo rimuove dalla lista originale\n        selected_img = available_images.pop(random_index)\n        \n        # 3. Aggiungilo alla lista train\n        train_imgs.append(selected_img)\n\n    # --- ESTRAZIONE VALIDATION SET ---\n    print(f\"--- Estrazione casuale di {num_val} immagini per il VALIDATION ---\")\n    for i in range(num_val):\n        # Pesco dalle rimanenti\n        random_index = random.randint(0, len(available_images) - 1)\n        selected_img = available_images.pop(random_index)\n        val_imgs.append(selected_img)\n\n    # --- ESTRAZIONE TEST SET ---\n    print(f\"--- Estrazione casuale di {num_test} immagini per il TEST ---\")\n    for i in range(num_test):\n        # Pesco dalle rimanenti\n        random_index = random.randint(0, len(available_images) - 1)\n        selected_img = available_images.pop(random_index)\n        test_imgs.append(selected_img)\n        \n    # --- RIEPILOGO ---\n    print(f\"\\nRiepilogo Assegnazioni:\")\n    print(f\"TRAIN: {len(train_imgs)} immagini\")\n    print(f\"VAL:   {len(val_imgs)} immagini\")\n    print(f\"TEST:  {len(test_imgs)} immagini\")\n    print(f\"RIMASTE (non usate): {len(available_images)} immagini\")\n\n    # --- GENERAZIONE DEI FILE ---\n    print(\"\\n--- Generazione Tiles Training ---\")\n    tile_dataset(train_imgs, TRAIN_DIR, TRAIN_JSON, TILE_SIZE, STRIDE, coords_by_img)\n    \n    print(\"--- Generazione Tiles Validation ---\")\n    tile_dataset(val_imgs, VAL_DIR, VAL_JSON, TILE_SIZE, STRIDE, coords_by_img)\n\n    print(\"--- Generazione Test Set (JSON) ---\")\n    with open(TEST_JSON, 'w') as f:\n        json.dump(test_imgs, f)\n    \n    test_ground_truth = {img: coords_by_img[img] for img in test_imgs}\n    with open(TEST_GT_JSON, 'w') as f:\n        json.dump(test_ground_truth, f)\n\n\ncreate_datasets(num_train=700, num_val=100, num_test=47)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:54:14.371562Z","iopub.execute_input":"2026-01-04T22:54:14.372247Z","iopub.status.idle":"2026-01-04T22:57:58.137645Z","shell.execute_reply.started":"2026-01-04T22:54:14.372215Z","shell.execute_reply":"2026-01-04T22:57:58.137028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modifica di Compose e ToTensor #\nModifico le seguenti classi per consentire il compose (e le transform) di img + annotations\nprima avevo infatti che Compose accettava solo 1 param. : l'img, ora devo flippare anche le annotations","metadata":{}},{"cell_type":"code","source":"# 1. Classe Compose personalizzata che accetta (image, target)\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\n# 2. ToTensor che accetta (image, target)\nclass ToTensor(object):\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n\n# 3. Flip Orizzontale (Immagine + Box)\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            \n            # Flippo l'immagine\n            image = image.flip(-1)\n            \n            # Flippo i box\n            bbox = target[\"boxes\"]\n            # x1_new = width - x2_old\n            # x2_new = width - x1_old\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n        return image, target\n\n# 4. Flip Verticale (Immagine + Box) - Fondamentale per il satellite\nclass RandomVerticalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            \n            # Flippo l'immagine\n            image = image.flip(-2)\n            \n            # Flippo i box\n            bbox = target[\"boxes\"]\n            # y1_new = height - y2_old\n            # y2_new = height - y1_old\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n        return image, target\n\n# classe color Jitter per uniformare contrasto\nclass ColorJitter(object):\n    def __init__(self, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05):\n        # Usiamo l'implementazione veloce di PyTorch internamente\n        self.transform = T.ColorJitter(brightness=brightness, \n                                       contrast=contrast, \n                                       saturation=saturation, \n                                       hue=hue)\n\n    def __call__(self, image, target):\n        # Applico il jitter SOLO all'immagine\n        image = self.transform(image)\n        \n        # Restituisco l'immagine modificata e il target (box) INALTERATO\n        return image, target\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:05.644644Z","iopub.execute_input":"2026-01-04T22:58:05.644949Z","iopub.status.idle":"2026-01-04T22:58:05.654556Z","shell.execute_reply.started":"2026-01-04T22:58:05.644907Z","shell.execute_reply":"2026-01-04T22:58:05.653858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classe Dataset #","metadata":{}},{"cell_type":"code","source":"class MioxViewDataset(Dataset):\n    def __init__(self, root_dir, annotation_file, transforms=None):\n        self.root_dir = root_dir\n        self.transforms = transforms\n        \n        # Carico il file delle annotazioni\n        with open(annotation_file, 'r') as f:\n            self.imgs_data = json.load(f)\n\n        # Mapping delle classi\n        self.class_map = {11: 1, 12: 2, 13: 3, 15: 4, 17: 5, 18: 6, 19: 7, 20: 8,\n            21: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14, 28: 15, 29: 16,\n            32: 17, 33: 18, 34: 19, 35: 20, 36: 21, 37: 22, 38: 23, 40: 24,\n            41: 25, 42: 26, 44: 27, 45: 28, 47: 29, 49: 30, 50: 31, 51: 32,\n            52: 33, 53: 34, 54: 35, 55: 36, 56: 37, 57: 38, 59: 39, 60: 40,\n            61: 41, 62: 42, 63: 43, 64: 44, 65: 45, 66: 46, 71: 47, 72: 48,\n            73: 49, 74: 50, 76: 51, 77: 52, 79: 53, 83: 54, 84: 55, 86: 56,\n            89: 57, 91: 58, 93: 59, 94: 60}\n\n    def __len__(self):\n        return len(self.imgs_data)\n\n    def __getitem__(self, idx):\n        data = self.imgs_data[idx]\n        img_path = os.path.join(self.root_dir, data['img_name'])\n        \n        # Caricamento immagine\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        boxes = []\n        labels = []\n        \n        # --- COSTANTI PER FILTRAGGIO ---\n        MIN_AREA = 16 * 16  # 256 pixel quadrati\n        MIN_DIM = 4         # Almeno 4px di lato\n        # -------------------------------\n\n        for obj in data['objects']:\n            xmin, ymin, xmax, ymax = obj['bounds']\n            \n            # Calcolo dimensioni\n            width = xmax - xmin\n            height = ymax - ymin\n            area = width * height\n\n            # 1. Filtro Validità base\n            if xmax <= xmin or ymax <= ymin:\n                continue\n\n            # 2. Filtro Dimensioni Minime (< 4px)\n            # Mask R-CNN crasha se i box sono degeneri o troppo piccoli per le ancore\n            if width < MIN_DIM or height < MIN_DIM:\n                continue\n\n            # 3. Filtro Area Minima (< 16x16) e Troncamento indiretto\n            # Se un oggetto è troncato pesantemente, la sua area crollerà.\n            # Questo check rimuove i frammenti piccoli ai bordi.\n            if area < MIN_AREA:\n                continue\n\n            # 4. Check Label (Rimuovi classe 0 o non mappata)\n            original_id = obj['type_id']\n            label_id = self.class_map.get(original_id, 0) \n            \n            if label_id == 0:\n                continue\n\n            # Se passa tutti i test, aggiungi\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(label_id)\n\n        # Conversione in Tensori\n        target = {}\n        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n        target[\"image_id\"] = torch.tensor([idx])\n        \n        # Area e Iscrowd (Standard COCO)\n        if len(boxes) > 0:\n            target[\"area\"] = (target[\"boxes\"][:, 3] - target[\"boxes\"][:, 1]) * (target[\"boxes\"][:, 2] - target[\"boxes\"][:, 0])\n        else:\n            target[\"area\"] = torch.zeros((0,), dtype=torch.float32)\n            \n        target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n\n        # Gestione TILE VUOTE \n        # (Se dopo il filtraggio rigoroso 16x16 non è rimasto nulla)\n        if len(boxes) == 0:\n            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n            target[\"labels\"] = torch.zeros((0,), dtype=torch.int64)\n            target[\"area\"] = torch.zeros((0,), dtype=torch.float32)\n\n        if self.transforms:\n            img, target = self.transforms(img, target)\n        else:\n            img = F.to_tensor(img)\n\n        return img, target\n\n# Collate function rimane uguale\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:10.306902Z","iopub.execute_input":"2026-01-04T22:58:10.307486Z","iopub.status.idle":"2026-01-04T22:58:10.320438Z","shell.execute_reply.started":"2026-01-04T22:58:10.307458Z","shell.execute_reply":"2026-01-04T22:58:10.319600Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modello #","metadata":{}},{"cell_type":"markdown","source":"Modifico la resnet per fare transfer learning","metadata":{}},{"cell_type":"code","source":"def get_maskrcnn_model(num_classes):\n\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n            weights=\"DEFAULT\",\n            min_size=512, # fisso la dimensione del resize automatico della rete (default 1333x800), altrimenti la rete effettua train su dimensioni diverse che dal test\n            max_size=512, # analogo\n            rpn_fg_iou_thresh=0.4, # considera roi se > 0.4\n            rpn_bg_iou_thresh=0.2, # considera sfondo se < 0.2\n        \n            # aumento le proposte per essere sicuro di non perderne\n            rpn_pre_nms_top_n_train=2000, \n            rpn_post_nms_top_n_train=2000,\n            # ----------------------------------\n        )    \n\n    #le ancore della rete addestrata su COCO sono troppo grandi (32x32) rispetto a quelle che servono per xView -> inserisco anche 8x8 e 16x16 \n    \n    # Definiamo le tue ancore personalizzate per xView\n    \n    anchor_sizes = ((8,), (16,), (32,), (64,), (128,))\n    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n    \n    # Creiamo il nuovo generatore\n    rpn_anchor_generator = AnchorGenerator(\n        sizes=anchor_sizes, \n        aspect_ratios=aspect_ratios\n    )\n    \n    model.rpn.anchor_generator = rpn_anchor_generator\n\n    # SOSTITUZIONE  DELLA TESTA DELLA RPN \n    # Devo ricreare la \"head\" perché ora deve predire 15 ancore invece di 3.\n    # Calcoliamo i canali di input (normalmente 256 per ResNet50-FPN)\n    in_channels = model.rpn.head.cls_logits.in_channels\n    \n    # Calcoliamo quante ancore ci sono per ogni pixel (5 sizes * 3 ratios = 15)\n    num_anchors = rpn_anchor_generator.num_anchors_per_location()[0]\n    \n    # Creiamo una nuova testa RPN fresca e pulita\n    model.rpn.head = RPNHead(in_channels, num_anchors)\n    \n    # 2. CAMBIA LA TESTA DEI BOX (Rettangoli + Classificazione)\n    in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features_box, num_classes)\n\n    # 3. CAMBIA LA TESTA DELLE MASCHERE (Segmentation), non ci serve a nulla -> None\n    model.roi_heads.mask_predictor = None # fondamentale\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:14.286986Z","iopub.execute_input":"2026-01-04T22:58:14.287774Z","iopub.status.idle":"2026-01-04T22:58:14.293781Z","shell.execute_reply.started":"2026-01-04T22:58:14.287742Z","shell.execute_reply":"2026-01-04T22:58:14.293082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transforms per data augmentation #\n\nAltrimenti il modello funziona male (prove precedenti)","metadata":{}},{"cell_type":"code","source":"def get_train_transform():\n    return Compose([\n        ToTensor(),                 # 1. Img -> Tensore\n        \n        # 2. Data Augmentation Colore (Solo Train)\n        # Valori consigliati per satellite:\n        # - Brightness/Contrast: 0.2-0.3 (gestisce ombre e luci forti)\n        # - Hue: basso (0.05) perché un prato verde non diventa mai viola\n        ColorJitter(),\n        # 3. Data Augmentation Geometrica\n        RandomHorizontalFlip(0.5),  \n        RandomVerticalFlip(0.5)     \n    ])\n\ndef get_val_transform():\n    return Compose([\n        ToTensor(), # In validazione niente Jitter né Flip!\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:36.888429Z","iopub.execute_input":"2026-01-04T22:58:36.889065Z","iopub.status.idle":"2026-01-04T22:58:36.893203Z","shell.execute_reply.started":"2026-01-04T22:58:36.889036Z","shell.execute_reply":"2026-01-04T22:58:36.892439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Dataset e Loader di TRAINING\ntrain_dataset = MioxViewDataset(\n    root_dir='/kaggle/working/train_tiles',\n    annotation_file='/kaggle/working/train_annotations.json',\n    transforms=get_train_transform()\n)\ntrain_loader = DataLoader(\n    train_dataset, batch_size=8, shuffle=True, \n    num_workers=2, collate_fn= collate_fn\n)\n\n# 2. Dataset e Loader di VALIDATION\nval_dataset = MioxViewDataset(\n    root_dir='/kaggle/working/val_tiles',\n    annotation_file='/kaggle/working/val_annotations.json',\n    transforms=get_val_transform()\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=8, shuffle=False, \n    num_workers=2, collate_fn=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:42.674811Z","iopub.execute_input":"2026-01-04T22:58:42.675220Z","iopub.status.idle":"2026-01-04T22:58:43.985503Z","shell.execute_reply.started":"2026-01-04T22:58:42.675188Z","shell.execute_reply":"2026-01-04T22:58:43.984800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Check delle tile dal train_loader #","metadata":{}},{"cell_type":"code","source":"# Prendi un batch dal loader\nimages, targets = next(iter(train_loader))\n\n# Prendi la prima immagine\nimg = images[0]\ntarget = targets[0]\n\nimg = img.permute(1, 2, 0).numpy() # C,H,W -> H,W,C\nimg = np.clip(img, 0, 1)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(img)\nax = plt.gca()\n\nfor box in target['boxes']:\n    x1, y1, x2, y2 = box.numpy()\n    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, color='red', linewidth=2)\n    ax.add_patch(rect)\n\nplt.title(f\"Check Training Data: {len(target['boxes'])} boxes\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:56:37.371988Z","iopub.execute_input":"2026-01-04T12:56:37.372227Z","iopub.status.idle":"2026-01-04T12:56:43.047724Z","shell.execute_reply.started":"2026-01-04T12:56:37.372204Z","shell.execute_reply":"2026-01-04T12:56:43.046710Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Check classi nel val_dataset #","metadata":{}},{"cell_type":"code","source":"TRAIN_ID_TO_NAME = {\n    1: 'Fixed-wing Aircraft', 2: 'Small Aircraft', 3: 'Cargo Plane', 4: 'Helicopter',\n    5: 'Passenger Vehicle', 6: 'Small Car', 7: 'Bus', 8: 'Pickup Truck',\n    9: 'Utility Truck', 10: 'Truck', 11: 'Cargo Truck', 12: 'Truck w/Box',\n    13: 'Truck Tractor', 14: 'Trailer', 15: 'Truck w/Flatbed', 16: 'Truck w/Liquid',\n    17: 'Crane Truck', 18: 'Railway Vehicle', 19: 'Passenger Car', 20: 'Cargo Car',\n    21: 'Flat Car', 22: 'Tank Car', 23: 'Locomotive', 24: 'Maritime Vessel',\n    25: 'Motorboat', 26: 'Sailboat', 27: 'Tugboat', 28: 'Barge',\n    29: 'Fishing Vessel', 30: 'Ferry', 31: 'Yacht', 32: 'Container Ship',\n    33: 'Oil Tanker', 34: 'Engineering Vehicle', 35: 'Tower crane', 36: 'Container Crane',\n    37: 'Reach Stacker', 38: 'Straddle Carrier', 39: 'Mobile Crane', 40: 'Dump Truck',\n    41: 'Haul Truck', 42: 'Scraper/Tractor', 43: 'Front loader/Bulldozer', 44: 'Excavator',\n    45: 'Cement Mixer', 46: 'Ground Grader', 47: 'Hut/Tent', 48: 'Shed',\n    49: 'Building', 50: 'Aircraft Hangar', 51: 'Damaged Building', 52: 'Facility',\n    53: 'Construction Site', 54: 'Vehicle Lot', 55: 'Helipad', 56: 'Storage Tank', 57: 'Shipping Container Lot',\n    58: 'Shipping container', 59: 'Pylon', 60: 'Tower'\n}\n\n# Conta le classi nel validation set\nval_class_counts = {}\nfor idx in range(len(val_dataset)):\n    _, target = val_dataset[idx]\n    labels = target['labels'].numpy()\n    for l in labels:\n        val_class_counts[l] = val_class_counts.get(l, 0) + 1\n\nprint(\"Classi presenti nel Validation Set:\")\nfor class_id, count in sorted(val_class_counts.items()):\n    name = TRAIN_ID_TO_NAME.get(class_id, str(class_id))\n    print(f\"{name}: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:59:03.329705Z","iopub.execute_input":"2026-01-04T22:59:03.330511Z","iopub.status.idle":"2026-01-04T22:59:15.625709Z","shell.execute_reply.started":"2026-01-04T22:59:03.330478Z","shell.execute_reply":"2026-01-04T22:59:15.624958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training rete #","metadata":{}},{"cell_type":"code","source":"# --- CONFIGURAZIONE INIZIALE ---\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 1. Caricamento Modello (Assicurati che get_maskrcnn_model abbia mask_predictor=None)\nmodel = get_maskrcnn_model(num_classes=61) \nmodel.to(device)\n\n\n# 2. Congelamento Pesi\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Blocca C1 (Stem iniziale)\nfor param in model.backbone.body.conv1.parameters():\n    param.requires_grad = False\nfor param in model.backbone.body.bn1.parameters():\n    param.requires_grad = False\n\n# Blocca C2 (Layer 1)\nfor param in model.backbone.body.layer1.parameters():\n    param.requires_grad = False\n\n# 3. Setup Optimizer e Scheduler\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-2)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n# 4. Setup Metrica MAP\n# class_metrics=False ci dà la media totale. Se metti True, ti dà il mAP per ogni singola classe (molto verboso)\nmetric = MeanAveragePrecision(iou_type=\"bbox\", class_metrics=True).to(device)\n\n# Storico\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'train_box_loss': [],\n    'val_map_50': [], # Aggiungiamo lo storico del mAP\n    'val_recall': []\n}\n\nnum_epochs = 25\n\n# --- MAPPA ID -> NOME (Necessaria per la stampa dei log) ---\n# Questo dizionario mappa l'ID sequenziale del modello (1-60) al Nome Reale\n\n\nprint(\"Inizio Training...\")\n\nfor epoch in range(num_epochs):\n    # ==========================\n    # 1. FASE DI TRAINING\n    # ==========================\n    model.train()\n    running_loss = 0.0\n    running_box_loss = 0.0\n    \n    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n    \n    for images, targets in train_loop:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Forward pass (in train mode restituisce le loss)\n        loss_dict = model(images, targets)\n\n        # Somma delle loss (nota: loss_mask non esiste più, quindi non serve azzerarla)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        # Monitoraggio specifico box loss\n        box_loss = loss_dict['loss_box_reg']\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        # Accumulo\n        running_loss += losses.item()\n        running_box_loss += box_loss.item()\n        \n        # Aggiorno la barra di caricamento con la loss attuale\n        train_loop.set_postfix(loss=losses.item())\n\n    # Medie Training\n    avg_train_loss = running_loss / len(train_loader)\n    avg_box_loss = running_box_loss / len(train_loader)\n    \n    history['train_loss'].append(avg_train_loss)\n    history['train_box_loss'].append(avg_box_loss)\n\n    # ==========================\n    # 2. FASE DI VALIDATION (Loss)\n    # ==========================\n    # TRUCCO: Per avere la LOSS, dobbiamo restare in model.train()\n    # ma usiamo torch.no_grad() per non addestrare.\n    # Questo perché model.eval() NON restituisce la loss in PyTorch Detection.\n    val_running_loss = 0.0\n    \n    # Usiamo solo una parte del validation set per la loss se è troppo lento,\n    # altrimenti usalo tutto. Qui lo uso tutto.\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            val_running_loss += losses.item()\n            \n    avg_val_loss = val_running_loss / len(val_loader)\n    history['val_loss'].append(avg_val_loss)\n\n    # ==========================\n    # 3. FASE DI VALIDATION (Metriche: mAP, Recall)\n    # ==========================\n    model.eval() # ORA passiamo in modalità valutazione per le predizioni\n    \n    with torch.no_grad():\n        for images, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Eval]\"):\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            # Qui il modello restituisce PREDIZIONI (boxes, scores, labels)\n            predictions = model(images)\n            \n            # Aggiorna la metrica accumulando i dati\n            metric.update(predictions, targets)\n\n    # Calcolo finale delle metriche per questa epoca\n    # ... dopo metric.update(predictions, targets) nel loop di eval ...\n\n    # Calcolo finale delle metriche\n    val_metrics = metric.compute()\n    \n    # Metriche Globali\n    map_50 = val_metrics['map_50'].item()\n    recall = val_metrics['mar_100'].item()\n    \n    # --- NUOVO: GESTIONE PER CLASSE ---\n    # map_per_class è un tensore. Lo convertiamo in lista CPU.\n    map_per_class = val_metrics['map_per_class'] \n    \n    # 1. Stampa Globale\n    print(f\"\\n EPOCA {epoch+1}/{num_epochs}\")\n    print(f\"   mAP @ 50 (Global): {map_50:.4f}\")\n    print(f\"   Recall (Global):   {recall:.4f}\")\n    print(\"-\" * 40)\n    print(\" DETTAGLIO CLASSI (Top & Flop):\")\n\n    # 2. Creiamo una lista ordinata per vedere chi va bene e chi male\n    class_results = []\n    \n    # Iteriamo sul tensore. Nota: l'indice del tensore corrisponde al label ID.\n    # Attenzione: Faster R-CNN usa 0 come background. I tuoi ID partono da 1.\n    # Torchmetrics potrebbe restituire un array dove l'indice i corrisponde alla classe i.\n    for i, score in enumerate(map_per_class):\n        score_val = score.item()\n        \n        # Ignora classi con score -1 (significa che non c'erano nel validation set)\n        if score_val == -1: continue\n        \n        # Recupera il nome\n        # Se i tuoi label partono da 1, l'indice 0 del tensore potrebbe essere vuoto o background\n        class_name = TRAIN_ID_TO_NAME.get(i, f\"Class {i}\") \n        \n        class_results.append((class_name, score_val))\n\n    # Ordiniamo dal peggiore al migliore per visualizzare\n    class_results.sort(key=lambda x: x[1], reverse=True) # Dall'alto al basso\n\n    # Stampiamo le 5 migliori e le 5 peggiori per non intasare il log\n    for name, score in class_results[:5]:\n        print(f\"{name:<20}: {score:.4f}\")\n    \n    if len(class_results) > 10:\n        print(\"      ...\")\n        for name, score in class_results[-5:]:\n             print(f\"{name:<20}: {score:.4f}\")\n    else:\n        # Se sono poche stampale tutte\n        for name, score in class_results[5:]:\n             print(f\"      {name:<20}: {score:.4f}\")\n\n    # 3. Salvataggio nello storico (Solo globali per non fare un JSON gigante)\n    history['val_map_50'].append(map_50)\n    history['val_recall'].append(recall)\n    \n    # Se vuoi salvare tutto il dettaglio in un file separato per questa epoca:\n    # with open(f'/kaggle/working/metrics_epoch_{epoch+1}.txt', 'w') as f:\n    #     for name, score in class_results:\n    #         f.write(f\"{name}: {score}\\n\")\n\n    # Reset\n    metric.reset()\n\n    # ==========================\n    # 4. REPORT E SALVATAGGIO\n    # ==========================\n    lr_scheduler.step(avg_val_loss)\n    \n    print(f\"\\n EPOCA {epoch+1}/{num_epochs}\")\n    print(f\"   Train Loss: {avg_train_loss:.4f} | Box Loss: {avg_box_loss:.4f}\")\n    print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n    print(f\"   mAP @ 50:   {map_50:.4f} (Obiettivo: > 0.5)\")\n    print(f\"   Recall:     {recall:.4f}\")\n    print(\"-\" * 40)\n    \n    # Salva checkpoint\n    torch.save(model.state_dict(), f'/kaggle/working/model_epoch_{epoch+1}.pth')\n    \n    # Salva history JSON aggiornata\n    with open('/kaggle/working/history.json', 'w') as f:\n        json.dump(history, f)\n\n    # Pulizia Memoria\n    del images, targets, predictions, loss_dict, losses\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"   Val Loss: {avg_val_loss:.4f} | Current LR: {current_lr:.2e}\")\n\nprint(\"Training completato!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:59:43.060816Z","iopub.execute_input":"2026-01-04T22:59:43.061606Z","execution_failed":"2026-01-04T23:00:22.204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizzazione Grafici #","metadata":{}},{"cell_type":"markdown","source":"def plot_metrics(history):\n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    plt.figure(figsize=(15, 6))\n\n    # --- GRAFICO 1: TRAIN vs VAL LOSS (Il più importante) ---\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n    plt.plot(epochs, history['val_loss'], 'r-o', label='Validation Loss')\n    plt.title('Total Loss: Train vs Validation')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # --- GRAFICO 2: DETTAGLIO COMPONENTI (Mask vs Box) ---\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history['train_mask_loss'], 'g--', label='Mask Loss (Forma)')\n    plt.plot(epochs, history['train_box_loss'], 'm--', label='Box Loss (Posizione)')\n    plt.title('Dettaglio Training: Mask vs Box')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n# Esegui la funzione\nplot_metrics(history)","metadata":{"execution":{"iopub.status.busy":"2025-12-31T01:32:02.865705Z","iopub.status.idle":"2025-12-31T01:32:02.866029Z","shell.execute_reply.started":"2025-12-31T01:32:02.865903Z","shell.execute_reply":"2025-12-31T01:32:02.865921Z"}}},{"cell_type":"markdown","source":"# TEST #","metadata":{}},{"cell_type":"markdown","source":"Funzione di test (inferenza del modello)","metadata":{}},{"cell_type":"markdown","source":"def predict_large_image(model, img_path, device, chip_size=512, stride=400, score_threshold=0.5):\n\n    # 1. Carica l'immagine gigante\n    img_full = cv2.imread(img_path)\n    img_full = cv2.cvtColor(img_full, cv2.COLOR_BGR2RGB)\n    h, w, _ = img_full.shape\n    \n    # Liste per salvare tutte le predizioni parziali\n    all_boxes = []\n    all_scores = []\n    all_labels = []\n    \n    model.eval() # IMPORTANTE: mette il modello in modalità valutazione\n    \n    # 2. Sliding Window Loop\n    print(f\"Processando immagine {w}x{h}...\")\n    \n    with torch.no_grad(): # Niente gradienti per risparmiare memoria\n        for y in range(0, h, STRIDE):\n            for x in range(0, w, STRIDE):\n                # Coordinate del chip\n                x2 = min(x + TILE_SIZE, w)\n                y2 = min(y + TILE_SIZE, h)\n                x1 = x2 - TILE_SIZE\n                y1 = y2 - TILE_SIZE\n                \n                # Fix bordi negativi\n                if x1 < 0: x1 = 0\n                if y1 < 0: y1 = 0\n                \n                # Estrai il chip\n                tile = img_full[y1:y2, x1:x2]\n                \n                # Prepara per il modello (ToTensor + Batch dimension)\n                img_tensor = F.to_tensor(tile).to(device)\n                img_tensor = img_tensor.unsqueeze(0) # Diventa [1, C, H, W]\n                \n                # --- PREDIZIONE ---\n                prediction = model(img_tensor)[0]\n                \n                # Filtra per score (es. tieni solo se sicurezza > 50%)\n                mask_score = prediction['scores'] > score_threshold\n                boxes = prediction['boxes'][mask_score].cpu().numpy()\n                scores = prediction['scores'][mask_score].cpu().numpy()\n                labels = prediction['labels'][mask_score].cpu().numpy()\n                \n                # --- TRASFORMAZIONE COORDINATE (Da Locali a Globali) ---\n                # Aggiungiamo l'offset (x1, y1) a ogni box\n                for i in range(len(boxes)):\n                    boxes[i][0] += x1 # xmin\n                    boxes[i][1] += y1 # ymin\n                    boxes[i][2] += x1 # xmax\n                    boxes[i][3] += y1 # ymax\n                    \n                    all_boxes.append(boxes[i])\n                    all_scores.append(scores[i])\n                    all_labels.append(labels[i])\n\n    # Se non ha trovato nulla, ritorna vuoto\n    if len(all_boxes) == 0:\n        return np.array([]), np.array([]), np.array([])\n\n    # 3. GLOBAL NMS (Pulizia duplicati)\n    # Convertiamo tutto in tensori per usare la funzione NMS di PyTorch\n    all_boxes_t = torch.tensor(np.array(all_boxes)).float().to(device)\n    all_scores_t = torch.tensor(np.array(all_scores)).float().to(device)\n    all_labels_t = torch.tensor(np.array(all_labels)).int().to(device)\n    \n    # Applica NMS (iou_threshold=0.3 significa: se due box si sovrappongono per più del 30%, cancella quello più debole)\n    keep_indices = torchvision.ops.nms(all_boxes_t, all_scores_t, iou_threshold=0.3)\n    \n    # Risultato finale pulito\n    final_boxes = all_boxes_t[keep_indices].cpu().numpy()\n    final_scores = all_scores_t[keep_indices].cpu().numpy()\n    final_labels = all_labels_t[keep_indices].cpu().numpy()\n    \n    return final_boxes, final_scores, final_labels","metadata":{"execution":{"iopub.status.busy":"2025-12-31T01:32:02.867195Z","iopub.status.idle":"2025-12-31T01:32:02.867512Z","shell.execute_reply.started":"2025-12-31T01:32:02.867338Z","shell.execute_reply":"2025-12-31T01:32:02.867362Z"}}},{"cell_type":"markdown","source":"# Esempio di come usare il Test Set alla fine\nwith open('/kaggle/working/test_annotations.json') as f:\n    test_images_names = json.load(f)\n\n# Prendi la prima immagine di test\ntest_img_name = test_images_names[0]\nfull_path = f\"/kaggle/input/xview-dataset/train_images/train_images/{test_img_name}\"\n\n# Lancia la predizione sull'immagine intera\nboxes, scores, labels = predict_large_image(model, full_path, device, ...)\n\n\n\n\n\n# 1. Carica i pesi migliori salvati durante il training\n# model = get_maskrcnn_model(num_classes=61) # Ricrea l'architettura\n# model.load_state_dict(torch.load(\"model_epoch_2.pth\")) # Carica i pesi\n# model.to('cuda')\n\n# 2. Scegli un'immagine di test\ntest_image_path = '/kaggle/input/xview-dataset/train_images/train_images/1045.tif' # O una cartella di test vera\n\n# 3. Esegui la predizione\npred_boxes, pred_scores, pred_labels = predict_large_image(\n    model, \n    test_image_path, \n    device='cuda',\n    chip_size=512,\n    stride=400,    # Stride < Chip_size per l'overlap!\n    score_threshold=0.5\n)\n\nprint(f\"Trovati {len(pred_boxes)} oggetti nell'immagine intera!\")\n\n# 4. Visualizzazione (Plot)\n# Carichiamo l'immagine per disegnarci sopra\nimg = cv2.imread(test_image_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nfig, ax = plt.subplots(1, figsize=(12, 12))\nax.imshow(img)\n\n# Disegna ogni box\nfor box in pred_boxes:\n    xmin, ymin, xmax, ymax = box\n    w_box = xmax - xmin\n    h_box = ymax - ymin\n    \n    # Crea rettangolo rosso\n    rect = patches.Rectangle(\n        (xmin, ymin), w_box, h_box, \n        linewidth=2, edgecolor='r', facecolor='none'\n    )\n    ax.add_patch(rect)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-12-31T01:32:02.868925Z","iopub.status.idle":"2025-12-31T01:32:02.869254Z","shell.execute_reply.started":"2025-12-31T01:32:02.869097Z","shell.execute_reply":"2025-12-31T01:32:02.869113Z"}}}]}