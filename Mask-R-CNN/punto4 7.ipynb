{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2571636,"sourceType":"datasetVersion","datasetId":1561333}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTS # ","metadata":{}},{"cell_type":"code","source":"import json\nimport gc\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as T\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\nimport numpy as np\nimport cv2\nimport json\nimport os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm \nfrom pathlib import Path\nimport torch\nimport cv2\nimport numpy as np\nimport json\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport random\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport shutil\nfrom torchvision.models.detection.rpn import AnchorGenerator,RPNHead\nfrom tqdm.notebook import tqdm\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:53:34.425628Z","iopub.execute_input":"2026-01-04T22:53:34.426262Z","iopub.status.idle":"2026-01-04T22:53:51.358528Z","shell.execute_reply.started":"2026-01-04T22:53:34.426234Z","shell.execute_reply":"2026-01-04T22:53:51.357896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Studio di xView ","metadata":{}},{"cell_type":"code","source":"geojson_path = '/kaggle/input/xview-dataset/train_labels/xView_train.geojson' \nwith open(geojson_path) as f:\n    data = json.load(f)\n\nclass_ids = [feature['properties']['type_id'] for feature in data['features']]\n\ndf_classes = pd.DataFrame(class_ids, columns=['type_id'])\n\ncounts = df_classes['type_id'].value_counts().sort_index()\n\nnum_classes = df_classes['type_id'].nunique()\n\nprint(f\"Totale classi: {num_classes}\")\nprint(f\"Totale oggetti annotati: {len(df_classes)}\")\nprint(\"\\n--- Conteggio per Class ID ---\")\nprint(df_classes['type_id'].value_counts().sort_index(ascending=False).head(60)) #altrimenti me ne lista solamente 10 (5 top e 5 bottom)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:50:45.981102Z","iopub.execute_input":"2026-01-04T12:50:45.981515Z","iopub.status.idle":"2026-01-04T12:50:55.025118Z","shell.execute_reply.started":"2026-01-04T12:50:45.981490Z","shell.execute_reply":"2026-01-04T12:50:55.024523Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tiling di xView # ","metadata":{}},{"cell_type":"markdown","source":"# Funzione per prelevare immagini xview #\n\ntrasformo il geojson in un json normale (per ogni img)","metadata":{}},{"cell_type":"code","source":"def load_xview_data(geojson_path): # prendo geojson e creo un json/dizionario organizzato per id dell'img\n\n    print(\"Caricamento GeoJSON... (può richiedere un po')\")\n    with open(geojson_path) as f:\n        data = json.load(f)\n    \n    coords_by_img = {}\n    \n    for feature in tqdm(data['features']):\n        props = feature['properties']\n        img_name = props['image_id']\n        bounds = [int(float(x)) for x in props['bounds_imcoords'].split(',')]\n        type_id = props['type_id']\n        \n        if img_name not in coords_by_img:\n            coords_by_img[img_name] = [] # dizionario di liste (key: img_name -> value: lista di annotazioni)\n            \n        coords_by_img[img_name].append({\n            'bounds': bounds,\n            'type_id': type_id\n        })\n        \n    return coords_by_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:54:05.550729Z","iopub.execute_input":"2026-01-04T22:54:05.551668Z","iopub.status.idle":"2026-01-04T22:54:05.557088Z","shell.execute_reply.started":"2026-01-04T22:54:05.551635Z","shell.execute_reply":"2026-01-04T22:54:05.556275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Funzione di Tiling #\nUsata per entrambi i dataset (train,val)","metadata":{}},{"cell_type":"code","source":"def tile_dataset(image_list, output_dir, output_json_path, TILE_SIZE, STRIDE, coords_by_img):\n    subset_data = []\n    \n    MIN_AREA = 16 * 16  \n    MAX_TRUNCATION = 0.50 \n    MIN_DIM = 4 \n    # -------------------------------\n\n    for img_name in tqdm(image_list):\n        img_path = os.path.join(INPUT_IMAGES_DIR, img_name)\n        img = cv2.imread(img_path)\n        \n        if img is None: \n            continue\n                \n        h, w, _ = img.shape\n        original_boxes = coords_by_img[img_name]\n\n        # Tiling Loop\n        for y in range(0, h, STRIDE):\n            for x in range(0, w, STRIDE):\n                x2 = min(x + TILE_SIZE, w)\n                y2 = min(y + TILE_SIZE, h)\n                x1 = x2 - TILE_SIZE\n                y1 = y2 - TILE_SIZE\n                if x1 < 0: x1 = 0 \n                if y1 < 0: y1 = 0\n                \n                # Estrai la tile\n                tile_img = img[y1:y2, x1:x2]\n                \n                valid_objects = []\n                for obj in original_boxes:\n                    bx1, by1, bx2, by2 = obj['bounds']\n                    \n                    # Calcolo Area Originale (per il troncamento)\n                    original_w = bx2 - bx1\n                    original_h = by2 - by1\n                    original_area = original_w * original_h\n                    \n                    # Intersezione (Box nel Tile)\n                    ix1 = max(x1, bx1)\n                    iy1 = max(y1, by1)\n                    ix2 = min(x2, bx2) \n                    iy2 = min(y2, by2)\n                    \n                    if ix1 < ix2 and iy1 < iy2:\n                        # Coordinate locali nella tile\n                        nx1 = ix1 - x1\n                        ny1 = iy1 - y1\n                        nx2 = ix2 - x1\n                        ny2 = iy2 - y1\n                        \n                        new_w = nx2 - nx1\n                        new_h = ny2 - ny1\n                        new_area = new_w * new_h\n                        \n                        \n                        # Filtro Dimensione Minima (Pixel)\n                        if new_w < MIN_DIM or new_h < MIN_DIM:\n                            continue # Troppo sottile/piccolo\n                            \n                        # Filtro Area Minima\n                        if new_area < MIN_AREA:\n                            continue # Oggetto troppo piccolo per essere riconosciuto\n                            \n                        # 3. Filtro Troncamento \n                        visible_ratio = new_area / original_area\n                        truncation = 1.0 - visible_ratio\n                        \n                        if truncation > MAX_TRUNCATION:\n                            continue # Oggetto troppo tagliato\n                        \n                        # Se passa i filtri\n                        valid_objects.append({\n                            'bounds': [nx1, ny1, nx2, ny2],\n                            'type_id': obj['type_id']\n                        })\n                \n                # Salva solo se la tile ha oggetti validi sopravvissuti ai filtri\n                if len(valid_objects) > 0:\n                    tile_filename = f\"{Path(img_name).stem}_tile_{x}_{y}.jpg\"\n                    save_path = os.path.join(output_dir, tile_filename)\n                    cv2.imwrite(save_path, tile_img)\n                    subset_data.append({\n                        'img_name': tile_filename,\n                        'objects': valid_objects\n                    })\n\n    print(f\"Creati {len(subset_data)} tiles validi.\")\n                    \n    with open(output_json_path, 'w') as f:\n        json.dump(subset_data, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:54:10.886136Z","iopub.execute_input":"2026-01-04T22:54:10.886640Z","iopub.status.idle":"2026-01-04T22:54:10.897356Z","shell.execute_reply.started":"2026-01-04T22:54:10.886610Z","shell.execute_reply":"2026-01-04T22:54:10.896556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CONFIGURAZIONE ---\nINPUT_IMAGES_DIR = '/kaggle/input/xview-dataset/train_images/train_images' \nINPUT_GEOJSON = '/kaggle/input/xview-dataset/train_labels/xView_train.geojson'\n\n# Cartelle di output\nTRAIN_DIR = '/kaggle/working/train_tiles'\nVAL_DIR = '/kaggle/working/val_tiles'\nTRAIN_JSON = '/kaggle/working/train_annotations.json'\nVAL_JSON = '/kaggle/working/val_annotations.json'\nTEST_JSON = '/kaggle/working/test_annotations.json'\nTEST_GT_JSON = '/kaggle/working/test_gt_annotations.json'\n\n# Pulizia cartelle\nif os.path.exists(TRAIN_DIR): shutil.rmtree(TRAIN_DIR)\nif os.path.exists(VAL_DIR): shutil.rmtree(VAL_DIR)\nos.makedirs(TRAIN_DIR)\nos.makedirs(VAL_DIR)\n\nTILE_SIZE = 512\nSTRIDE = 400\n\ndef create_datasets(num_train, num_val, num_test):\n    \n    # 1. Carico TUTTE le immagini disponibili\n    print(\"Caricamento lista immagini...\")\n    coords_by_img = load_xview_data(INPUT_GEOJSON) \n    \n    available_images = list(coords_by_img.keys()) \n    total_available = len(available_images)\n    \n    print(f\"Totale immagini nell'urna: {total_available}\")\n    \n    # Controllo sicurezza\n    if (num_train + num_val + num_test) > total_available:\n        print(\"ERRORE: Hai chiesto più immagini di quelle disponibili!\")\n        return\n\n    # Liste vuote da riempire\n    train_imgs = []\n    val_imgs = []\n    test_imgs = []\n\n    # ESTRAZIONE TRAINING SET \n    print(f\"\\n--- Estrazione casuale di {num_train} immagini per il TRAIN ---\")\n    random.seed(42) \n    \n    for i in range(num_train):\n        # Genera un indice casuale tra 0 e l'ultimo indice disponibile rimasto\n        random_index = random.randint(0, len(available_images) - 1)\n        \n        selected_img = available_images.pop(random_index)\n        \n        train_imgs.append(selected_img)\n\n    # ESTRAZIONE VALIDATION SET\n    print(f\"--- Estrazione casuale di {num_val} immagini per il VALIDATION ---\")\n    for i in range(num_val):\n        random_index = random.randint(0, len(available_images) - 1)\n        selected_img = available_images.pop(random_index)\n        val_imgs.append(selected_img)\n\n    # ESTRAZIONE TEST SET \n    print(f\"--- Estrazione casuale di {num_test} immagini per il TEST ---\")\n    for i in range(num_test):\n        # Pesco dalle rimanenti\n        random_index = random.randint(0, len(available_images) - 1)\n        selected_img = available_images.pop(random_index)\n        test_imgs.append(selected_img)\n        \n    print(f\"\\nRiepilogo Assegnazioni:\")\n    print(f\"TRAIN: {len(train_imgs)} immagini\")\n    print(f\"VAL:   {len(val_imgs)} immagini\")\n    print(f\"TEST:  {len(test_imgs)} immagini\")\n    print(f\"RIMASTE (non usate): {len(available_images)} immagini\")\n\n    # --- GENERAZIONE DEI FILE ---\n    print(\"\\n--- Generazione Tiles Training ---\")\n    tile_dataset(train_imgs, TRAIN_DIR, TRAIN_JSON, TILE_SIZE, STRIDE, coords_by_img)\n    \n    print(\"--- Generazione Tiles Validation ---\")\n    tile_dataset(val_imgs, VAL_DIR, VAL_JSON, TILE_SIZE, STRIDE, coords_by_img)\n\n    print(\"--- Generazione Test Set (JSON) ---\")\n    with open(TEST_JSON, 'w') as f:\n        json.dump(test_imgs, f)\n    \n    test_ground_truth = {img: coords_by_img[img] for img in test_imgs}\n    with open(TEST_GT_JSON, 'w') as f:\n        json.dump(test_ground_truth, f)\n\n\ncreate_datasets(num_train=700, num_val=100, num_test=47)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:54:14.371562Z","iopub.execute_input":"2026-01-04T22:54:14.372247Z","iopub.status.idle":"2026-01-04T22:57:58.137645Z","shell.execute_reply.started":"2026-01-04T22:54:14.372215Z","shell.execute_reply":"2026-01-04T22:57:58.137028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modifica di Compose e ToTensor #\nModifico le seguenti classi per consentire il compose (e le transform) di img + annotations\nprima avevo infatti che Compose accettava solo 1 param. : l'img, ora devo flippare anche le annotations","metadata":{}},{"cell_type":"code","source":"# Classe Compose personalizzata che accetta (image, target)\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\n# ToTensor che accetta (image, target)\nclass ToTensor(object):\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n\n# Flip Orizzontale (Immagine + Box)\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            \n            # Flippo l'immagine\n            image = image.flip(-1)\n            \n            # Flippo i box\n            bbox = target[\"boxes\"]\n            # x1_new = width - x2_old\n            # x2_new = width - x1_old\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n        return image, target\n\n# Flip Verticale (Immagine + Box) - Fondamentale per il satellite\nclass RandomVerticalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            \n            # Flippo l'immagine\n            image = image.flip(-2)\n            \n            # Flippo i box\n            bbox = target[\"boxes\"]\n            # y1_new = height - y2_old\n            # y2_new = height - y1_old\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n        return image, target\n\n# classe color Jitter per uniformare contrasto\nclass ColorJitter(object):\n    def __init__(self, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05):\n        # Usiamo l'implementazione veloce di PyTorch internamente\n        self.transform = T.ColorJitter(brightness=brightness, \n                                       contrast=contrast, \n                                       saturation=saturation, \n                                       hue=hue)\n\n    def __call__(self, image, target):\n        # Applico il jitter SOLO all'immagine\n        image = self.transform(image)\n        \n        # Restituisco l'immagine modificata e il target (box) INALTERATO\n        return image, target\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:05.644644Z","iopub.execute_input":"2026-01-04T22:58:05.644949Z","iopub.status.idle":"2026-01-04T22:58:05.654556Z","shell.execute_reply.started":"2026-01-04T22:58:05.644907Z","shell.execute_reply":"2026-01-04T22:58:05.653858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classe Dataset #","metadata":{}},{"cell_type":"code","source":"class MioxViewDataset(Dataset):\n    def __init__(self, root_dir, annotation_file, transforms=None):\n        self.root_dir = root_dir\n        self.transforms = transforms\n        \n        # Carico il file delle annotazioni\n        with open(annotation_file, 'r') as f:\n            self.imgs_data = json.load(f)\n\n        # Mapping delle classi\n        self.class_map = {11: 1, 12: 2, 13: 3, 15: 4, 17: 5, 18: 6, 19: 7, 20: 8,\n            21: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14, 28: 15, 29: 16,\n            32: 17, 33: 18, 34: 19, 35: 20, 36: 21, 37: 22, 38: 23, 40: 24,\n            41: 25, 42: 26, 44: 27, 45: 28, 47: 29, 49: 30, 50: 31, 51: 32,\n            52: 33, 53: 34, 54: 35, 55: 36, 56: 37, 57: 38, 59: 39, 60: 40,\n            61: 41, 62: 42, 63: 43, 64: 44, 65: 45, 66: 46, 71: 47, 72: 48,\n            73: 49, 74: 50, 76: 51, 77: 52, 79: 53, 83: 54, 84: 55, 86: 56,\n            89: 57, 91: 58, 93: 59, 94: 60}\n\n    def __len__(self):\n        return len(self.imgs_data)\n\n    def __getitem__(self, idx):\n        data = self.imgs_data[idx]\n        img_path = os.path.join(self.root_dir, data['img_name'])\n        \n        # Caricamento immagine\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        boxes = []\n        labels = []\n        \n        # --- COSTANTI PER FILTRAGGIO ---\n        MIN_AREA = 16 * 16  \n        MIN_DIM = 4         \n        # -------------------------------\n\n        for obj in data['objects']:\n            xmin, ymin, xmax, ymax = obj['bounds']\n            \n            # Calcolo dimensioni\n            width = xmax - xmin\n            height = ymax - ymin\n            area = width * height\n\n            # Filtro Validità base\n            if xmax <= xmin or ymax <= ymin:\n                continue\n\n            # Filtro Dimensioni Minime (< 4px)\n            if width < MIN_DIM or height < MIN_DIM:\n                continue\n\n            # Filtro Area Minima (< 16x16) e Troncamento indiretto\n            if area < MIN_AREA:\n                continue\n\n            # Check Label (Rimuovi classe 0 o non mappata)\n            original_id = obj['type_id']\n            label_id = self.class_map.get(original_id, 0) \n            \n            if label_id == 0:\n                continue\n\n            # Se passa tutti i test, aggiungi\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(label_id)\n\n        # Conversione in Tensori\n        target = {}\n        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n        target[\"image_id\"] = torch.tensor([idx])\n        \n        # Area e Iscrowd (Standard COCO)\n        if len(boxes) > 0:\n            target[\"area\"] = (target[\"boxes\"][:, 3] - target[\"boxes\"][:, 1]) * (target[\"boxes\"][:, 2] - target[\"boxes\"][:, 0])\n        else:\n            target[\"area\"] = torch.zeros((0,), dtype=torch.float32)\n            \n        target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n\n        # Gestione TILE VUOTE \n        if len(boxes) == 0:\n            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n            target[\"labels\"] = torch.zeros((0,), dtype=torch.int64)\n            target[\"area\"] = torch.zeros((0,), dtype=torch.float32)\n\n        if self.transforms:\n            img, target = self.transforms(img, target)\n        else:\n            img = F.to_tensor(img)\n\n        return img, target\n\n# Collate function\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:10.306902Z","iopub.execute_input":"2026-01-04T22:58:10.307486Z","iopub.status.idle":"2026-01-04T22:58:10.320438Z","shell.execute_reply.started":"2026-01-04T22:58:10.307458Z","shell.execute_reply":"2026-01-04T22:58:10.319600Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modello #","metadata":{}},{"cell_type":"markdown","source":"Modifico la resnet per fare transfer learning","metadata":{}},{"cell_type":"code","source":"def get_maskrcnn_model(num_classes):\n\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n            weights=\"DEFAULT\",\n            min_size=512, # fisso la dimensione del resize automatico della rete (default 1333x800), altrimenti la rete effettua train su dimensioni diverse che dal test\n            max_size=512, # analogo\n            rpn_fg_iou_thresh=0.4, # considera roi se > 0.4\n            rpn_bg_iou_thresh=0.2, # considera sfondo se < 0.2\n        \n            # aumento le proposte per essere sicuro di non perderne\n            rpn_pre_nms_top_n_train=2000, \n            rpn_post_nms_top_n_train=2000,\n            # ----------------------------------\n        )    \n\n    #le ancore della rete addestrata su COCO sono troppo grandi (32x32) rispetto a quelle che servono per xView -> inserisco anche 8x8 e 16x16 \n        \n    anchor_sizes = ((8,), (16,), (32,), (64,), (128,))\n    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n    \n    rpn_anchor_generator = AnchorGenerator(\n        sizes=anchor_sizes, \n        aspect_ratios=aspect_ratios\n    )\n    \n    model.rpn.anchor_generator = rpn_anchor_generator\n\n    # SOSTITUZIONE  DELLA TESTA DELLA RPN \n    # Devo ricreare la \"head\" perché ora deve predire 15 ancore invece di 3.\n    in_channels = model.rpn.head.cls_logits.in_channels\n    \n    # Calcoliamo quante ancore ci sono per ogni pixel (5 sizes * 3 ratios = 15)\n    num_anchors = rpn_anchor_generator.num_anchors_per_location()[0]\n    \n    model.rpn.head = RPNHead(in_channels, num_anchors)\n    \n    # CAMBIO LA TESTA DEI BOX (Rettangoli + Classificazione)\n    in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features_box, num_classes)\n\n    # CAMBIO LA TESTA DELLE MASCHERE (Segmentation), non ci serve a nulla -> None\n    model.roi_heads.mask_predictor = None # fondamentale\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:14.286986Z","iopub.execute_input":"2026-01-04T22:58:14.287774Z","iopub.status.idle":"2026-01-04T22:58:14.293781Z","shell.execute_reply.started":"2026-01-04T22:58:14.287742Z","shell.execute_reply":"2026-01-04T22:58:14.293082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transforms per data augmentation #\n\nAltrimenti il modello funziona male (prove precedenti)","metadata":{}},{"cell_type":"code","source":"def get_train_transform():\n    return Compose([\n        ToTensor(),                 # 1. Img -> Tensore\n        \n        # Data Augmentation Colore (Solo Train)\n        # - Brightness/Contrast: 0.2-0.3 (gestisce ombre e luci forti)\n        # - Hue: basso (0.05) perché un prato verde non diventa mai viola\n        ColorJitter(),\n        # 3. Data Augmentation Geometrica\n        RandomHorizontalFlip(0.5),  \n        RandomVerticalFlip(0.5)     \n    ])\n\ndef get_val_transform():\n    return Compose([\n        ToTensor(), # In validazione niente Jitter né Flip\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:36.888429Z","iopub.execute_input":"2026-01-04T22:58:36.889065Z","iopub.status.idle":"2026-01-04T22:58:36.893203Z","shell.execute_reply.started":"2026-01-04T22:58:36.889036Z","shell.execute_reply":"2026-01-04T22:58:36.892439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset e Loader di TRAINING\ntrain_dataset = MioxViewDataset(\n    root_dir='/kaggle/working/train_tiles',\n    annotation_file='/kaggle/working/train_annotations.json',\n    transforms=get_train_transform()\n)\ntrain_loader = DataLoader(\n    train_dataset, batch_size=8, shuffle=True, \n    num_workers=2, collate_fn= collate_fn\n)\n\n# Dataset e Loader di VALIDATION\nval_dataset = MioxViewDataset(\n    root_dir='/kaggle/working/val_tiles',\n    annotation_file='/kaggle/working/val_annotations.json',\n    transforms=get_val_transform()\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=8, shuffle=False, \n    num_workers=2, collate_fn=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:58:42.674811Z","iopub.execute_input":"2026-01-04T22:58:42.675220Z","iopub.status.idle":"2026-01-04T22:58:43.985503Z","shell.execute_reply.started":"2026-01-04T22:58:42.675188Z","shell.execute_reply":"2026-01-04T22:58:43.984800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Check delle tile dal train_loader #","metadata":{}},{"cell_type":"code","source":"# Prendo un batch dal loader\nimages, targets = next(iter(train_loader))\n\n# Prendo la prima immagine\nimg = images[0]\ntarget = targets[0]\n\nimg = img.permute(1, 2, 0).numpy() # C,H,W -> H,W,C\nimg = np.clip(img, 0, 1)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(img)\nax = plt.gca()\n\nfor box in target['boxes']:\n    x1, y1, x2, y2 = box.numpy()\n    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, color='red', linewidth=2)\n    ax.add_patch(rect)\n\nplt.title(f\"Check Training Data: {len(target['boxes'])} boxes\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:56:37.371988Z","iopub.execute_input":"2026-01-04T12:56:37.372227Z","iopub.status.idle":"2026-01-04T12:56:43.047724Z","shell.execute_reply.started":"2026-01-04T12:56:37.372204Z","shell.execute_reply":"2026-01-04T12:56:43.046710Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Check classi nel val_dataset #","metadata":{}},{"cell_type":"code","source":"TRAIN_ID_TO_NAME = {\n    1: 'Fixed-wing Aircraft', 2: 'Small Aircraft', 3: 'Cargo Plane', 4: 'Helicopter',\n    5: 'Passenger Vehicle', 6: 'Small Car', 7: 'Bus', 8: 'Pickup Truck',\n    9: 'Utility Truck', 10: 'Truck', 11: 'Cargo Truck', 12: 'Truck w/Box',\n    13: 'Truck Tractor', 14: 'Trailer', 15: 'Truck w/Flatbed', 16: 'Truck w/Liquid',\n    17: 'Crane Truck', 18: 'Railway Vehicle', 19: 'Passenger Car', 20: 'Cargo Car',\n    21: 'Flat Car', 22: 'Tank Car', 23: 'Locomotive', 24: 'Maritime Vessel',\n    25: 'Motorboat', 26: 'Sailboat', 27: 'Tugboat', 28: 'Barge',\n    29: 'Fishing Vessel', 30: 'Ferry', 31: 'Yacht', 32: 'Container Ship',\n    33: 'Oil Tanker', 34: 'Engineering Vehicle', 35: 'Tower crane', 36: 'Container Crane',\n    37: 'Reach Stacker', 38: 'Straddle Carrier', 39: 'Mobile Crane', 40: 'Dump Truck',\n    41: 'Haul Truck', 42: 'Scraper/Tractor', 43: 'Front loader/Bulldozer', 44: 'Excavator',\n    45: 'Cement Mixer', 46: 'Ground Grader', 47: 'Hut/Tent', 48: 'Shed',\n    49: 'Building', 50: 'Aircraft Hangar', 51: 'Damaged Building', 52: 'Facility',\n    53: 'Construction Site', 54: 'Vehicle Lot', 55: 'Helipad', 56: 'Storage Tank', 57: 'Shipping Container Lot',\n    58: 'Shipping container', 59: 'Pylon', 60: 'Tower'\n}\n\n# Conto le classi nel validation set\nval_class_counts = {}\nfor idx in range(len(val_dataset)):\n    _, target = val_dataset[idx]\n    labels = target['labels'].numpy()\n    for l in labels:\n        val_class_counts[l] = val_class_counts.get(l, 0) + 1\n\nprint(\"Classi presenti nel Validation Set:\")\nfor class_id, count in sorted(val_class_counts.items()):\n    name = TRAIN_ID_TO_NAME.get(class_id, str(class_id))\n    print(f\"{name}: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:59:03.329705Z","iopub.execute_input":"2026-01-04T22:59:03.330511Z","iopub.status.idle":"2026-01-04T22:59:15.625709Z","shell.execute_reply.started":"2026-01-04T22:59:03.330478Z","shell.execute_reply":"2026-01-04T22:59:15.624958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training rete #","metadata":{}},{"cell_type":"code","source":"# CONFIGURAZIONE INIZIALE\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Caricamento Modello\nmodel = get_maskrcnn_model(num_classes=61) \nmodel.to(device)\n\n\n# Congelamento Pesi\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Blocco C1 \nfor param in model.backbone.body.conv1.parameters():\n    param.requires_grad = False\nfor param in model.backbone.body.bn1.parameters():\n    param.requires_grad = False\n\n# Blocco C2 (Layer 1)\nfor param in model.backbone.body.layer1.parameters():\n    param.requires_grad = False\n\n# 3. Setup Optimizer e Scheduler\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-2)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n# 4. Setup Metrica MAP\nmetric = MeanAveragePrecision(iou_type=\"bbox\", class_metrics=True).to(device)\n\n# Storico\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'train_box_loss': [],\n    'val_map_50': [], \n    'val_recall': []\n}\n\nnum_epochs = 25\n\n\nprint(\"Inizio Training...\")\n\nfor epoch in range(num_epochs):\n    # ==========================\n    # 1. FASE DI TRAINING\n    # ==========================\n    model.train()\n    running_loss = 0.0\n    running_box_loss = 0.0\n    \n    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n    \n    for images, targets in train_loop:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Forward pass \n        loss_dict = model(images, targets)\n\n        # Somma delle loss \n        losses = sum(loss for loss in loss_dict.values())\n        \n        box_loss = loss_dict['loss_box_reg']\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        # Accumulo\n        running_loss += losses.item()\n        running_box_loss += box_loss.item()\n        \n        # Aggiorno la barra di caricamento con la loss attuale\n        train_loop.set_postfix(loss=losses.item())\n\n    # Medie Training\n    avg_train_loss = running_loss / len(train_loader)\n    avg_box_loss = running_box_loss / len(train_loader)\n    \n    history['train_loss'].append(avg_train_loss)\n    history['train_box_loss'].append(avg_box_loss)\n\n    # ==========================\n    # 2. FASE DI VALIDATION (Loss)\n# Uso torch.no_grad() siccome non posso passare in modalità eval (non mi dà loss altrim), devo rimanere in train\n    val_running_loss = 0.0\n    \n\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            val_running_loss += losses.item()\n            \n    avg_val_loss = val_running_loss / len(val_loader)\n    history['val_loss'].append(avg_val_loss)\n\n    # ==========================\n    # 3. FASE DI VALIDATION \n    # ==========================\n    model.eval() # ORA passiamo in modalità valutazione per le predizioni\n    \n    with torch.no_grad():\n        for images, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Eval]\"):\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            # Predizioni\n            predictions = model(images)\n            \n            # Aggiorno la metrica accumulando i dati\n            metric.update(predictions, targets)\n\n\n    val_metrics = metric.compute()\n    \n    map_50 = val_metrics['map_50'].item()\n    recall = val_metrics['mar_100'].item()\n    \n\n    map_per_class = val_metrics['map_per_class'] \n    \n    # Stampa Globale\n    print(f\"\\n EPOCA {epoch+1}/{num_epochs}\")\n    print(f\"   mAP @ 50 (Global): {map_50:.4f}\")\n    print(f\"   Recall (Global):   {recall:.4f}\")\n    print(\"-\" * 40)\n    print(\" DETTAGLIO CLASSI (Top & Flop):\")\n\n    # Creiamo una lista ordinata per vedere chi va bene e chi male\n    class_results = []\n\n    for i, score in enumerate(map_per_class):\n        score_val = score.item()\n        \n        # Ignora classi con score -1 (che non c'erano nel validation set)\n        if score_val == -1: continue\n        \n        # Recupera il nome\n        class_name = TRAIN_ID_TO_NAME.get(i, f\"Class {i}\") \n        \n        class_results.append((class_name, score_val))\n\n    class_results.sort(key=lambda x: x[1], reverse=True) # Dall'alto al basso\n\n    for name, score in class_results[:5]:\n        print(f\"{name:<20}: {score:.4f}\")\n    \n    if len(class_results) > 10:\n        print(\"      ...\")\n        for name, score in class_results[-5:]:\n             print(f\"{name:<20}: {score:.4f}\")\n    else:\n        for name, score in class_results[5:]:\n             print(f\"      {name:<20}: {score:.4f}\")\n\n    # Salvataggio nello storico (Solo globali per non fare un JSON gigante)\n    history['val_map_50'].append(map_50)\n    history['val_recall'].append(recall)\n    \n\n    metric.reset()\n\n    # ==========================\n    # 4. REPORT E SALVATAGGIO\n    # ==========================\n    lr_scheduler.step(avg_val_loss)\n    \n    print(f\"\\n EPOCA {epoch+1}/{num_epochs}\")\n    print(f\"   Train Loss: {avg_train_loss:.4f} | Box Loss: {avg_box_loss:.4f}\")\n    print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n    print(f\"   mAP @ 50:   {map_50:.4f} (Obiettivo: > 0.5)\")\n    print(f\"   Recall:     {recall:.4f}\")\n    print(\"-\" * 40)\n    \n    # Salvo checkpoint\n    torch.save(model.state_dict(), f'/kaggle/working/model_epoch_{epoch+1}.pth')\n    \n    # Salvo history JSON aggiornata\n    with open('/kaggle/working/history.json', 'w') as f:\n        json.dump(history, f)\n\n    # Pulizia Memoria\n    del images, targets, predictions, loss_dict, losses\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"   Val Loss: {avg_val_loss:.4f} | Current LR: {current_lr:.2e}\")\n\nprint(\"Training completato!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T22:59:43.060816Z","iopub.execute_input":"2026-01-04T22:59:43.061606Z","execution_failed":"2026-01-04T23:00:22.204Z"}},"outputs":[],"execution_count":null}]}