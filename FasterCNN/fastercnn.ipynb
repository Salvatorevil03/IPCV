{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":14364890,"sourceType":"datasetVersion","datasetId":9172930}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\n\nprint(\"File scaricati con successo!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:05.702767Z","iopub.execute_input":"2026-01-02T15:40:05.703074Z","iopub.status.idle":"2026-01-02T15:40:06.864780Z","shell.execute_reply.started":"2026-01-02T15:40:05.703047Z","shell.execute_reply":"2026-01-02T15:40:06.863900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch-lr-finder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:08.510248Z","iopub.execute_input":"2026-01-02T15:40:08.510582Z","iopub.status.idle":"2026-01-02T15:40:12.745821Z","shell.execute_reply.started":"2026-01-02T15:40:08.510552Z","shell.execute_reply":"2026-01-02T15:40:12.744914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.datasets import CocoDetection\nfrom torchvision.ops import box_convert\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data import Subset\nimport os\nimport json\nfrom PIL import Image\nfrom engine import train_one_epoch, evaluate\nfrom torch_lr_finder import LRFinder\nfrom PIL import Image\nfrom engine import train_one_epoch, evaluate\nimport utils\nfrom coco_utils import get_coco_api_from_dataset\nfrom coco_eval import CocoEvaluator\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/coco-2017-dataset/coco2017\"\nTRAIN_IMAGES = f\"{DATASET_PATH}/train2017\"\nVAL_IMAGES = f\"{DATASET_PATH}/val2017\"\nTRAIN_ANN = f\"{DATASET_PATH}/annotations/instances_train2017.json\"\nVAL_ANN = f\"{DATASET_PATH}/annotations/instances_val2017.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:17.622286Z","iopub.execute_input":"2026-01-02T15:40:17.622598Z","iopub.status.idle":"2026-01-02T15:40:17.626924Z","shell.execute_reply.started":"2026-01-02T15:40:17.622572Z","shell.execute_reply":"2026-01-02T15:40:17.626205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform_train = transforms.Compose([ \n    # 1. Color Jitter: Simula diverse condizioni di luce (sole, ombra, ecc.)\n    # È la tecnica pixel-level più efficace.\n    transforms.ColorJitter(\n        brightness=0.2,  # Varia la luminosità del ±20%\n        contrast=0.2,    # Varia il contrasto del ±20%\n        saturation=0.2,  # Varia la saturazione del ±20%\n        hue=0.05         # Leggera variazione della tonalità\n    ),\n\n    # 2. Random Grayscale: Rende l'immagine in b/n il 10% delle volte\n    # Aiuta il modello a non basarsi solo sui colori specifici\n    transforms.RandomGrayscale(p=0.1),\n\n    # 4. Conversione finale in Tensor\n    transforms.ToTensor()\n])\n\ntransform_val = transforms.Compose([\n    transforms.ToTensor()    \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:18.757063Z","iopub.execute_input":"2026-01-02T15:40:18.757405Z","iopub.status.idle":"2026-01-02T15:40:18.762320Z","shell.execute_reply.started":"2026-01-02T15:40:18.757378Z","shell.execute_reply":"2026-01-02T15:40:18.761718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CocoToPyTorchFormat:\n    def __call__(self, target):\n        # 1. Estrai i box grezzi (xywh)\n        boxes_xywh = [obj['bbox'] for obj in target]\n        boxes_xywh = torch.as_tensor(boxes_xywh, dtype=torch.float32)\n        \n        # in_fmt='xywh' -> out_fmt='xyxy'\n        boxes_xyxy = box_convert(boxes_xywh, in_fmt='xywh', out_fmt='xyxy')\n\n        # 3. Estrai le altre info\n        labels = [obj['category_id'] for obj in target]\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        area = torch.as_tensor([obj['area'] for obj in target], dtype=torch.float32)\n        iscrowd = torch.as_tensor([obj['iscrowd'] for obj in target], dtype=torch.int64)\n        \n        image_id = torch.tensor([target[0]['image_id']])\n        \n        return {\n            \"boxes\": boxes_xyxy,\n            \"labels\": labels,\n            \"image_id\": image_id,\n            \"area\": area,\n            \"iscrowd\": iscrowd\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:24.510237Z","iopub.execute_input":"2026-01-02T15:40:24.510585Z","iopub.status.idle":"2026-01-02T15:40:24.516696Z","shell.execute_reply.started":"2026-01-02T15:40:24.510557Z","shell.execute_reply":"2026-01-02T15:40:24.515972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = CocoDetection(root=TRAIN_IMAGES, annFile=TRAIN_ANN, transform=transform_train,target_transform=CocoToPyTorchFormat())\nval_dataset = CocoDetection(root=VAL_IMAGES, annFile=VAL_ANN, transform=transform_val,target_transform=CocoToPyTorchFormat())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:25.547126Z","iopub.execute_input":"2026-01-02T15:40:25.547867Z","iopub.status.idle":"2026-01-02T15:40:43.411250Z","shell.execute_reply.started":"2026-01-02T15:40:25.547837Z","shell.execute_reply":"2026-01-02T15:40:43.410664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def carica_indici_da_json(filename=\"balanced_indices.json\"):\n    with open(filename, 'r', encoding='utf-8') as f:\n        indices = json.load(f)\n    print(f\"✅ Caricati {len(indices)} indici da {filename} (Formato JSON)\")\n    return indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:46.714818Z","iopub.execute_input":"2026-01-02T15:40:46.715081Z","iopub.status.idle":"2026-01-02T15:40:46.719420Z","shell.execute_reply.started":"2026-01-02T15:40:46.715059Z","shell.execute_reply":"2026-01-02T15:40:46.718695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"balanced_indices_train=carica_indici_da_json(\"/kaggle/input/indici/balanced_indices.json\")\nbalanced_indices_val=carica_indici_da_json(\"/kaggle/input/indici/balanced_indices_val.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:47.699082Z","iopub.execute_input":"2026-01-02T15:40:47.699627Z","iopub.status.idle":"2026-01-02T15:40:47.720004Z","shell.execute_reply.started":"2026-01-02T15:40:47.699597Z","shell.execute_reply":"2026-01-02T15:40:47.719437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_subset = Subset(train_dataset,balanced_indices_train )\nval_subset = Subset(val_dataset, balanced_indices_val)\nprint(f\"✅ TRAIN: {len(train_subset)} images\")\nprint(f\"✅ VAL:   Using {len(val_subset)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:49.072250Z","iopub.execute_input":"2026-01-02T15:40:49.072962Z","iopub.status.idle":"2026-01-02T15:40:49.077218Z","shell.execute_reply.started":"2026-01-02T15:40:49.072936Z","shell.execute_reply":"2026-01-02T15:40:49.076560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_subset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=lambda x: tuple(zip(*x))\n)\n\nval_loader = DataLoader(\n    val_subset,\n    batch_size=4,\n    shuffle=False,     \n    num_workers=0,\n    collate_fn=lambda x: tuple(zip(*x))\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:50.289703Z","iopub.execute_input":"2026-01-02T15:40:50.290006Z","iopub.status.idle":"2026-01-02T15:40:50.294743Z","shell.execute_reply.started":"2026-01-02T15:40:50.289980Z","shell.execute_reply":"2026-01-02T15:40:50.293948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model(num_classes=91):\n    # Load pretrained Faster R-CNN model (ResNet50 backbone)\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None, weights_backbone=torchvision.models.resnet.ResNet50_Weights.IMAGENET1K_V1)\n    \n    # Get input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    \n    # Replace with new head (COCO has 91 classes)\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:52.563942Z","iopub.execute_input":"2026-01-02T15:40:52.564457Z","iopub.status.idle":"2026-01-02T15:40:52.568549Z","shell.execute_reply.started":"2026-01-02T15:40:52.564429Z","shell.execute_reply":"2026-01-02T15:40:52.567833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch_lr_finder import LRFinder\nimport torch\n\nclass RetinaNetLRFinder(LRFinder):\n    def __init__(self, model, optimizer, criterion=None, device=None, memory_cache=True, cache_dir=None):\n        # RetinaNet calcola la loss internamente, quindi non serve un criterio esterno\n        super().__init__(model, optimizer, criterion, device, memory_cache, cache_dir)\n\n    def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n        self.model.train()\n        self.optimizer.zero_grad()\n\n        # Estraiamo il batch dal DataLoader\n        # Nota: RetinaNet richiede collate_fn, quindi data è (images, targets)\n        try:\n            images, targets = next(train_iter)\n        except StopIteration:\n            return None\n\n        # --- GESTIONE DATI CUSTOM PER RETINANET ---\n        # Spostiamo immagini e target su GPU manualmente perché sono liste/dizionari\n        images = list(image.to(self.device) for image in images)\n        targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n\n        # Forward pass (RetinaNet calcola la loss internamente)\n        loss_dict = self.model(images, targets)\n        \n        # Somma delle loss (Classificazione + Regressione)\n        loss = sum(loss for loss in loss_dict.values())\n\n        # Backward pass\n        loss.backward()\n        self.optimizer.step()\n\n        return loss.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:52.779743Z","iopub.execute_input":"2026-01-02T15:40:52.780429Z","iopub.status.idle":"2026-01-02T15:40:52.788886Z","shell.execute_reply.started":"2026-01-02T15:40:52.780402Z","shell.execute_reply":"2026-01-02T15:40:52.788204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n#model = get_model()\n#model.to(device)\n\n#optimizer = torch.optim.SGD(\n#    [p for p in model.parameters() if p.requires_grad],\n#    lr=1e-4,           # Start un po' più alto\n#    momentum=0.9,      # Importante per la stabilità\n#    weight_decay=0.0005\n#)\n\n#lr_finder = RetinaNetLRFinder(model, optimizer, device=device)\n\n#lr_finder.range_test(\n#    train_loader, \n#    start_lr=1e-5,     # Start: abbastanza basso da non esplodere subito\n#    end_lr=0.1,        # End: abbastanza alto da divergere sicuramente\n#    num_iter=100,      # Rapido: 100 batch sono sufficienti\n#    step_mode=\"exp\"\n#)\n\n#lr_finder.plot() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:40:55.025262Z","iopub.execute_input":"2026-01-02T15:40:55.025574Z","iopub.status.idle":"2026-01-02T15:42:17.351581Z","shell.execute_reply.started":"2026-01-02T15:40:55.025550Z","shell.execute_reply":"2026-01-02T15:42:17.350869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import matplotlib.pyplot as plt\n\n# 1. Salviamo la funzione originale 'show' in una variabile\n#original_show = plt.show\n\n# 2. Sostituiamo 'plt.show' con una funzione che non fa nulla (dummy)\n#    In questo modo, quando lr_finder chiama .show(), non succede nulla e il grafico resta in memoria!\n#plt.show = lambda: None\n\n#try:\n    # 3. Generiamo il grafico\n    # La libreria proverà a fare show(), ma noi l'abbiamo disattivata :)\n  #  lr_finder.plot(suggest_lr=True)\n\n    # 4. Ora il grafico è ancora \"vivo\". Salviamolo!\n#    plt.savefig('lr_finder_result.png', dpi=300, bbox_inches='tight')\n#    print(\"✅ Grafico salvato correttamente!\")\n\n#finally:\n    # 5. IMPORTANTE: Ripristiniamo la funzione show originale\n    # Così i futuri grafici funzioneranno normalmente\n#    plt.show = original_show\n\n# 6. Se vuoi vederlo anche a schermo adesso, chiamiamo la show originale\n#plt.show()\n\n# 7. Reset del finder\n#lr_finder.reset()“","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T15:48:08.239204Z","iopub.execute_input":"2026-01-02T15:48:08.239621Z","iopub.status.idle":"2026-01-02T15:48:08.753278Z","shell.execute_reply.started":"2026-01-02T15:48:08.239587Z","shell.execute_reply":"2026-01-02T15:48:08.752515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = get_model()\nmodel.to(device)\noptimizer = torch.optim.SGD(\n    model.parameters(),\n    lr=0.005,  # 5e-3\n    momentum=0.9,\n    weight_decay=0.0005\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T18:39:56.041888Z","iopub.execute_input":"2026-01-01T18:39:56.042253Z","iopub.status.idle":"2026-01-01T18:39:56.559948Z","shell.execute_reply.started":"2026-01-01T18:39:56.042227Z","shell.execute_reply":"2026-01-01T18:39:56.559342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 26\nsave_dir = \"/kaggle/working/\"\nmodel.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T00:36:18.851426Z","iopub.execute_input":"2025-12-30T00:36:18.851708Z","iopub.status.idle":"2025-12-30T00:36:18.856481Z","shell.execute_reply.started":"2025-12-30T00:36:18.851687Z","shell.execute_reply":"2025-12-30T00:36:18.855718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _get_iou_types(model):\n    model_without_ddp = model\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model_without_ddp = model.module\n    iou_types = [\"bbox\"]\n    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n        iou_types.append(\"segm\")\n    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n        iou_types.append(\"keypoints\")\n    return iou_types","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.inference_mode()\ndef myevaluate(model, data_loader, device):\n    n_threads = torch.get_num_threads()\n    # FIXME remove this and make paste_masks_in_image run on the GPU\n    torch.set_num_threads(1)\n    cpu_device = torch.device(\"cpu\")\n    model.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = \"Test:\"\n\n    coco = get_coco_api_from_dataset(data_loader.dataset)\n    iou_types = _get_iou_types(model)\n    coco_evaluator = CocoEvaluator(coco, iou_types)\n\n    for images, targets in metric_logger.log_every(data_loader, 100, header):\n        images = list(img.to(device) for img in images)\n\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        model_time = time.time()\n        outputs = model(images)\n\n        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n        evaluator_time = time.time()\n        coco_evaluator.update(res)\n        evaluator_time = time.time() - evaluator_time\n        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    coco_evaluator.synchronize_between_processes()\n\n    # accumulate predictions from all images\n    coco_evaluator.accumulate()\n    coco_evaluator.summarize()\n    torch.set_num_threads(n_threads)\n    return coco_evaluator","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom collections import defaultdict\n\n@torch.no_grad()\ndef evaluate_loss(model, data_loader, device):\n    # IMPORTANTE: Mettiamo il modello in train per calcolare le loss\n    model.train()\n    \n    # Usiamo defaultdict per accumulare automaticamente qualsiasi chiave trovi\n    metric_logger = defaultdict(float)\n    num_batches = 0\n    \n    for images, targets in data_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        # Forward pass\n        loss_dict = model(images, targets)\n        \n        # 1. Calcolo loss totale del batch\n        losses = sum(loss for loss in loss_dict.values())\n        \n        # 2. Accumulo loss totale\n        metric_logger['total_loss'] += losses.item()\n        \n        # 3. Accumulo AUTOMATICO di tutte le loss specifiche trovate\n        # (Funziona sia per RetinaNet che per Faster R-CNN senza cambiare codice)\n        for k, v in loss_dict.items():\n            metric_logger[k] += v.item()\n            \n        num_batches += 1\n        \n    if num_batches == 0:\n        return {}\n        \n    # Calcoliamo la media dividendo per il numero di batch\n    final_metrics = {k: v / num_batches for k, v in metric_logger.items()}\n    \n    return final_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from engine import train_one_epoch\nimport time\n\n# Inizializziamo la history\nhistory = []\n\nprint(f\"Inizio Training per {num_epochs} epoche (Solo Loss, No mAP)...\")\nstart_total = time.time()\n\nfor epoch in range(num_epochs):\n    epoch_start = time.time()\n    \n    # --- 1. TRAINING ---\n    metric_logger = train_one_epoch(model, optimizer, train_loader, device, epoch+1, print_freq=500)\n    \n    # --- 2. VALIDATION LOSS DETTAGLIATA ---\n    # Questa funzione ora restituisce il dizionario con TUTTE le loss\n    val_metrics = evaluate_loss(model, val_loader, device)\n    \n    # --- 3. STEP SCHEDULER ---\n    lr_scheduler.step()\n    current_lr = optimizer.param_groups[0][\"lr\"]\n\n    # --- 4. RACCOLTA DATI (Tutte le componenti) ---\n    epoch_log = {\n        'epoch': epoch,\n        'lr': current_lr,\n        \n        # TRAINING (Estraiamo singole componenti se servono, qui prendiamo loss totale e componenti)\n        'train_loss': metric_logger.meters['loss'].global_avg,\n        'train_loss_classifier': metric_logger.meters['loss_classifier'].global_avg,\n        'train_loss_box_reg': metric_logger.meters['loss_box_reg'].global_avg,\n        'train_loss_objectness': metric_logger.meters['loss_objectness'].global_avg,\n        'train_loss_rpn_box_reg': metric_logger.meters['loss_rpn_box_reg'].global_avg,\n        \n        # VALIDATION (Salviamo tutto separatamente)\n        'val_loss': val_metrics['total_loss'],                 # Media totale\n        'val_loss_classifier': val_metrics['loss_classifier'], # Errore classificazione\n        'val_loss_box_reg': val_metrics['loss_box_reg'],       # Errore posizione box\n        'val_loss_objectness': val_metrics['loss_objectness'], # Errore foreground/background\n        'val_loss_rpn_box_reg': val_metrics['loss_rpn_box_reg'] # Errore box RPN\n    }\n    \n    history.append(epoch_log)\n\n    # --- 5. STAMPA RECAP PULITA ---\n    epoch_duration = time.time() - epoch_start\n    print(\"-\" * 50)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] - {epoch_duration/60:.1f} min\")\n    print(f\"Train Loss: {epoch_log['train_loss']:.4f}\")\n    print(f\"Val Loss:   {epoch_log['val_loss']:.4f}  <-- (Cls: {epoch_log['val_loss_classifier']:.3f} | Box: {epoch_log['val_loss_box_reg']:.3f})\")\n    print(f\"LR:         {current_lr:.6f}\")\n    print(\"-\" * 50)\n    \n    # --- 6. SALVATAGGIO CHECKPOINT ---\n    checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch}.pth\")\n    torch.save({\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'lr_scheduler': lr_scheduler.state_dict(),\n        'epoch': epoch,\n        'history': history, # La history ora contiene tutte le loss dettagliate\n    }, checkpoint_path)\n    \n    print(f\"Salvato checkpoint: {checkpoint_path}\")\n\ntotal_time = time.time() - start_total\nprint(f\"Training Completato in {total_time/3600:.2f} ore! ✅\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}