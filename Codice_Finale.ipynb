{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-24T09:21:09.219323Z",
     "iopub.status.busy": "2025-11-24T09:21:09.218716Z",
     "iopub.status.idle": "2025-11-24T09:21:09.224608Z",
     "shell.execute_reply": "2025-11-24T09:21:09.224003Z",
     "shell.execute_reply.started": "2025-11-24T09:21:09.219300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms.functional as F  \n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import copy\n",
    "import numpy as np # linear algebra\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import albumentations as A  # our data augmentation library\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # progress bar\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import sys\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T07:51:50.796778Z",
     "iopub.status.busy": "2025-11-24T07:51:50.796043Z",
     "iopub.status.idle": "2025-11-24T07:51:50.806934Z",
     "shell.execute_reply": "2025-11-24T07:51:50.806195Z",
     "shell.execute_reply.started": "2025-11-24T07:51:50.796749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classe Dataset\n",
    "\"\"\"\n",
    "class MyReducedCocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation_file, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation_file) # Carica le annotazioni\n",
    "        all_ids = list(sorted(self.coco.imgs.keys())) ##mappo gli id in una lista ordinata\n",
    "        self.ids=[]\n",
    "\n",
    "        for img_id in tqdm(all_ids):\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "            full_path = os.path.join(self.root, path)\n",
    "            \n",
    "            if os.path.exists(full_path):\n",
    "                self.ids.append(img_id)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Carica ID e path immagine\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index] #mi prendo id effettivo immagine\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id) #mi prendo le annotations\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        \n",
    "        img = Image.open(os.path.join(self.root, path)).convert(\"RGB\") ##pytorch vuole solo img in rgb\n",
    "\n",
    "        # 3. Estrai le Box e le Label\n",
    "        #nbisogna convertire i bounding box\n",
    "        num_objs = len(coco_annotation)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2] # COCO è x,y,w,h -> convertiamo in x,y,x,y\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            \n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(coco_annotation[i]['category_id'])\n",
    "            areas.append(coco_annotation[i]['area'])\n",
    "            iscrowd.append(coco_annotation[i]['iscrowd'])\n",
    "\n",
    "        # Conversione in Tensori\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([img_id])\n",
    "        area = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Gestione casi senza box (immagini vuote)\n",
    "        if num_objs == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            img = F.to_tensor(img)\n",
    "    \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    #((img1,target1),(img2,target2)) ---- ((img1,img2),(target1,target2))\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creiamo le istanze della classe dataset\n",
    "e le diamo in input alla classe dataload\n",
    "\"\"\"\n",
    "IMG_DIR_TRAIN = '/kaggle/input/cococustom/kaggle/working/train'\n",
    "ANN_FILE_TRAIN = '/kaggle/input/cococustom/kaggle/working/instances_train2017.json'\n",
    "\n",
    "IMG_DIR_VAL = '/kaggle/input/cococustom/kaggle/working/val'\n",
    "ANN_FILE_VAL = '/kaggle/input/cococustom/kaggle/working/instances_val2017.json'\n",
    "\n",
    "dataset_train = MyReducedCocoDataset(root=IMG_DIR_TRAIN, annotation_file=ANN_FILE_TRAIN)\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, \n",
    "        batch_size=4, \n",
    "        shuffle=True,\n",
    "        num_workers=2, \n",
    "        collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "dataset_val = MyReducedCocoDataset(root=IMG_DIR_VAL, annotation_file=ANN_FILE_VAL)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, \n",
    "        batch_size=4, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RIDUZIONE DATASETS PER TEST\"\"\"\n",
    "# 1. Definisci quanto grande vuoi che sia il tuo mini-set (es. 50 immagini)\n",
    "mini_size = 50\n",
    "\n",
    "# 2. Crea una lista di indici (da 0 a 49)\n",
    "# Puoi anche usare indici random se preferisci un campione casuale\n",
    "indices = list(range(mini_size))\n",
    "\n",
    "# 3. Crea il Subset\n",
    "mini_dataset_train = Subset(dataset_train, indices)\n",
    "\n",
    "# 4. Crea il DataLoader specifico per il mini dataset\n",
    "mini_loader_train = torch.utils.data.DataLoader(\n",
    "    mini_dataset_train, \n",
    "    batch_size=4,           # Stesso batch size o diverso, come preferisci\n",
    "    shuffle=True,           # Shuffle è utile anche nel test per verificare che il modello non crashi\n",
    "    num_workers=2, \n",
    "    collate_fn=collate_fn   # Importante: mantieni la tua collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Dimensione dataset TRAIN originale: {len(dataset_train)}\")\n",
    "print(f\"Dimensione mini TRAIN dataset: {len(mini_dataset_train)}\")\n",
    "\n",
    "mini_dataset_val = Subset(dataset_val, indices)\n",
    "\n",
    "# 4. Crea il DataLoader specifico per il mini dataset\n",
    "mini_loader_val = torch.utils.data.DataLoader(\n",
    "    mini_dataset_val, \n",
    "    batch_size=4,           # Stesso batch size o diverso, come preferisci\n",
    "    shuffle=True,           # Shuffle è utile anche nel test per verificare che il modello non crashi\n",
    "    num_workers=2, \n",
    "    collate_fn=collate_fn   # Importante: mantieni la tua collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Dimensione dataset VAL originale: {len(dataset_train)}\")\n",
    "print(f\"Dimensione mini VAL dataset: {len(mini_dataset_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T14:46:41.892488Z",
     "iopub.status.busy": "2025-11-19T14:46:41.891877Z",
     "iopub.status.idle": "2025-11-19T14:46:43.268585Z",
     "shell.execute_reply": "2025-11-19T14:46:43.267942Z",
     "shell.execute_reply.started": "2025-11-19T14:46:41.892463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importiamo il modello Faster RCNN ci torchvision\n",
    "\"\"\"\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "\n",
    "\"\"\"\n",
    "Attivare la GPU\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-19T16:14:56.931Z",
     "iopub.execute_input": "2025-11-19T14:48:41.495099Z",
     "iopub.status.busy": "2025-11-19T14:48:41.494461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_smart(model, optimizer, train_loader, val_loader, device, num_epochs, save_path='.'):\n",
    "    \"\"\"\n",
    "    Funzione di training adattata per Object Detection con struttura avanzata:\n",
    "    - Training Loop\n",
    "    - Validation Loop\n",
    "    - Model Checkpointing (Best & Last)\n",
    "    - History return\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup history e variabili\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_sub_losses': [], # Per salvare classifier, box_reg, etc.\n",
    "        'val_sub_losses': []\n",
    "    }\n",
    "    \n",
    "    min_val_loss = np.inf\n",
    "    model.to(device)\n",
    "    \n",
    "    fit_time = time.time()\n",
    "    \n",
    "    print(f\"Inizio training su {device} per {num_epochs} epoche...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        since = time.time()\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_sub_losses = {} # Per accumulare le loss parziali (rpn, box, class, etc)\n",
    "        \n",
    "        # Nota: Se vuoi usare tqdm, avvolgi train_loader qui: tqdm(train_loader)\n",
    "        for i, (images, targets) in enumerate(train_loader):\n",
    "            \n",
    "            # Preparazione dati specifica per Object Detection\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (Il modello calcola la loss internamente se passiamo i targets)\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "            # Calcolo loss totale\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "\n",
    "            # Controllo stabilità\n",
    "            if not math.isfinite(loss_value):\n",
    "                print(f\"\\nLoss is {loss_value}, stopping training\")\n",
    "                print(loss_dict)\n",
    "                sys.exit(1)\n",
    "\n",
    "            # Backward & Step\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulo metriche\n",
    "            running_loss += loss_value\n",
    "            \n",
    "            # Accumulo sotto-loss per statistiche (staccandole dal grafo)\n",
    "            for k, v in loss_dict.items():\n",
    "                if k not in running_sub_losses:\n",
    "                    running_sub_losses[k] = 0.0\n",
    "                running_sub_losses[k] += v.item()\n",
    "\n",
    "        # Media epocale train\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_sub = {k: v / len(train_loader) for k, v in running_sub_losses.items()}\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        # ATTENZIONE: Per i modelli di Detection Torchvision, se metti model.eval(),\n",
    "        # il modello restituisce predizioni (box), NON loss. \n",
    "        # Per monitorare la val_loss, rimaniamo in .train() ma con torch.no_grad().\n",
    "        running_val_loss = 0.0\n",
    "        running_val_sub_losses = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # model.train() # Rimane tecnicamente in train mode per calcolare le loss\n",
    "            for images, targets in val_loader:\n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                running_val_loss += losses.item()\n",
    "                \n",
    "                for k, v in loss_dict.items():\n",
    "                    if k not in running_val_sub_losses:\n",
    "                        running_val_sub_losses[k] = 0.0\n",
    "                    running_val_sub_losses[k] += v.item()\n",
    "\n",
    "        # Media epocale val\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        epoch_val_sub = {k: v / len(val_loader) for k, v in running_val_sub_losses.items()}\n",
    "\n",
    "        # --- SALVATAGGIO STORICO ---\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['train_sub_losses'].append(epoch_train_sub)\n",
    "        history['val_sub_losses'].append(epoch_val_sub)\n",
    "\n",
    "        # --- CHECKPOINTING (Best Model) ---\n",
    "        if epoch_val_loss < min_val_loss:\n",
    "            print('Loss Decreasing.. {:.4f} >> {:.4f} (Saving Model)'.format(min_val_loss, epoch_val_loss))\n",
    "            min_val_loss = epoch_val_loss\n",
    "            torch.save(model, f'{save_path}/model_best.pt')\n",
    "\n",
    "        # --- STAMPA STATISTICHE ---\n",
    "        time_elapsed = time.time() - since\n",
    "        print(\"Epoch:{}/{} | Time: {:.0f}m {:.0f}s\".format(epoch + 1, num_epochs, time_elapsed // 60, time_elapsed % 60))\n",
    "        print(f\"Train Loss: {epoch_train_loss:.5f} | Val Loss: {epoch_val_loss:.5f}\")\n",
    "        # Stampa opzionale di una componente specifica (es. classifier loss)\n",
    "        # print(f\"Train Class Loss: {epoch_train_sub.get('loss_classifier', 0):.5f}\") \n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # --- FINE TRAINING ---\n",
    "    total_time = time.time() - fit_time\n",
    "    print('Total training time: {:.0f}m {:.0f}s'.format(total_time // 60, total_time % 60))\n",
    "    print('Saving last model...')\n",
    "    torch.save(model, f'{save_path}/model_last.pt')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# --- ESEMPIO DI UTILIZZO ---\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Assicurati di avere un data_loader_val definito\n",
    "history = train_smart(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    train_loader=data_loader_train, \n",
    "    val_loader=data_loader_val,  # Serve un validation loader\n",
    "    device=device, \n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Crea una nuova figura\n",
    "plt.figure(figsize=(10, 5)) # Opzionale: rende il grafico un po' più largo\n",
    "\n",
    "# Plot delle curve\n",
    "plt.plot(history['train_loss'], color='blue', label='Train Loss')\n",
    "plt.plot(history['val_loss'], color='red', label='Validation Loss')\n",
    "\n",
    "# Configurazioni\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Total Loss') # Corretto per Object Detection\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.grid(True, linestyle='--', alpha=0.6) # Opzionale: aggiunge una griglia per leggibilità\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T07:52:14.345588Z",
     "iopub.status.busy": "2025-11-24T07:52:14.344752Z",
     "iopub.status.idle": "2025-11-24T07:52:14.350544Z",
     "shell.execute_reply": "2025-11-24T07:52:14.349936Z",
     "shell.execute_reply.started": "2025-11-24T07:52:14.345503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calcola l’IoU tra due bounding box.\n",
    "    box formato: [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "\n",
    "    inter_w = max(0, xB - xA)\n",
    "    inter_h = max(0, yB - yA)\n",
    "    interArea = inter_w * inter_h\n",
    "\n",
    "    box1Area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2Area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    iou = interArea / float(box1Area + box2Area - interArea + 1e-6)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T07:53:12.749409Z",
     "iopub.status.busy": "2025-11-24T07:53:12.748691Z",
     "iopub.status.idle": "2025-11-24T07:53:12.754426Z",
     "shell.execute_reply": "2025-11-24T07:53:12.753767Z",
     "shell.execute_reply.started": "2025-11-24T07:53:12.749381Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.999999999375\n",
      "IoU: 0.14285714280612247\n",
      "IoU: 0.0\n"
     ]
    }
   ],
   "source": [
    "# sovrapposizione totale\n",
    "boxA = [10, 10, 50, 50]\n",
    "boxB = [10, 10, 50, 50]\n",
    "print(\"IoU:\", compute_iou(boxA, boxB))\n",
    "\n",
    "# sovrapposizione parziale\n",
    "boxA = [10, 10, 50, 50]\n",
    "boxB = [30, 30, 70, 70]\n",
    "print(\"IoU:\", compute_iou(boxA, boxB))\n",
    "\n",
    "# sovrapposizione assente\n",
    "boxA = [10, 10, 40, 40]\n",
    "boxB = [50, 50, 80, 80]\n",
    "print(\"IoU:\", compute_iou(boxA, boxB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T08:29:42.086833Z",
     "iopub.status.busy": "2025-11-24T08:29:42.085993Z",
     "iopub.status.idle": "2025-11-24T08:29:42.093775Z",
     "shell.execute_reply": "2025-11-24T08:29:42.093019Z",
     "shell.execute_reply.started": "2025-11-24T08:29:42.086805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Precision=TP/TP+FP\n",
    "Recall=TP/TP+FN\n",
    "\n",
    "Condizioni:\n",
    "1. Ha trovato l’oggetto nel punto giusto? Confronta ogni box predetto con quello reale usando la IoU\n",
    "2. Ha riconosciuto la classe giusta? Verifica che la label sia la stessa del Ground Truth\n",
    "Se entrambi sono veri: True Positive (TP)\n",
    "Se una delle due condizioni non è soddisfatta: False Positive (FP)\n",
    "Se un oggetto reale non viene mai trovato: False Negative (FN)\n",
    "\n",
    "NON è sufficiente per mAP\n",
    "per la quale serve fare Precision e Recall per ogni classe e per più soglie\n",
    "\"\"\"\n",
    "def compute_precision_recall_fasterrcnn(predictions, targets, iou_threshold=0.5):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for pred, gt in zip(predictions, targets):\n",
    "\n",
    "        pred_boxes = pred[\"boxes\"].cpu().numpy()\n",
    "        pred_labels = pred[\"labels\"].cpu().numpy()\n",
    "        pred_scores = pred[\"scores\"].cpu().numpy()\n",
    "\n",
    "        gt_boxes = gt[\"boxes\"].cpu().numpy()\n",
    "        gt_labels = gt[\"labels\"].cpu().numpy()\n",
    "\n",
    "        matched_gt = set()\n",
    "\n",
    "        # Ordina predizioni per score decrescente\n",
    "        sorted_indices = np.argsort(-pred_scores)\n",
    "        pred_boxes = pred_boxes[sorted_indices]\n",
    "        pred_labels = pred_labels[sorted_indices]\n",
    "\n",
    "        for pb, pl in zip(pred_boxes, pred_labels):\n",
    "\n",
    "            best_iou = 0\n",
    "            best_gt = -1\n",
    "\n",
    "            for i, (gt_b, gt_l) in enumerate(zip(gt_boxes, gt_labels)):\n",
    "                \n",
    "                if i in matched_gt:\n",
    "                    continue  # già assegnato a un TP\n",
    "                \n",
    "                if pl != gt_l:\n",
    "                    continue  # classe sbagliata\n",
    "\n",
    "                iou = compute_iou(pb, gt_b)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt = i\n",
    "\n",
    "            if best_iou >= iou_threshold and best_gt not in matched_gt:\n",
    "                tp += 1\n",
    "                matched_gt.add(best_gt)\n",
    "            else:\n",
    "                fp += 1\n",
    "        \n",
    "        fn += len(gt_boxes) - len(matched_gt)\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T08:29:45.756816Z",
     "iopub.status.busy": "2025-11-24T08:29:45.756282Z",
     "iopub.status.idle": "2025-11-24T08:29:45.764632Z",
     "shell.execute_reply": "2025-11-24T08:29:45.763957Z",
     "shell.execute_reply.started": "2025-11-24T08:29:45.756789Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.33333322222225925\n",
      "Recall: 0.499999750000125\n"
     ]
    }
   ],
   "source": [
    "predictions = [{\n",
    "    \"boxes\": torch.tensor([\n",
    "        [12, 12, 48, 48],   # match buono con la persona\n",
    "        [55, 55, 95, 95],   # box giusto sul cane, ma classe sbagliata\n",
    "        [0, 0, 20, 20],     # predizione su sfondo\n",
    "    ], dtype=torch.float32),\n",
    "    \"labels\": torch.tensor([\n",
    "        1,  # persona → CORRETTO\n",
    "        3,  # gatto → SBAGLIATO\n",
    "        1,  # persona → SBAGLIATO → FP\n",
    "    ]),\n",
    "    \"scores\": torch.tensor([\n",
    "        0.98, # molto alto\n",
    "        0.90,\n",
    "        0.30,\n",
    "    ])\n",
    "}]\n",
    "\n",
    "targets = [{\n",
    "    \"boxes\": torch.tensor([\n",
    "        [10, 10, 50, 50],   # persona\n",
    "        [60, 60, 100, 100], # cane\n",
    "    ], dtype=torch.float32),\n",
    "    \"labels\": torch.tensor([\n",
    "        1,  # classe persona\n",
    "        2,  # classe cane\n",
    "    ])\n",
    "}]\n",
    "\n",
    "precision, recall = compute_precision_recall_fasterrcnn(predictions, targets)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T09:25:49.041222Z",
     "iopub.status.busy": "2025-11-24T09:25:49.040648Z",
     "iopub.status.idle": "2025-11-24T09:25:49.047287Z",
     "shell.execute_reply": "2025-11-24T09:25:49.046502Z",
     "shell.execute_reply.started": "2025-11-24T09:25:49.041196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cosa misura la mAP?\n",
    "Misura quanto bene un modello:\n",
    "1. trova tutti gli oggetti corretti (completezza -> recall)\n",
    "2. evita falsi positivi (precisione -> precision)\n",
    "Come si ottiene la mAP?\n",
    "1. Per ogni classe si costruisce la Precision-Recall curve\n",
    "2. Si calcola l’Average Precision (AP) = area sotto quella curva\n",
    "3. Si calcola la media delle AP di tutte le classi -> Mean AP\n",
    "\n",
    "La curva di colcola al variare della soglia\n",
    "Che cos’è la “soglia”?\n",
    "Ogni predizione del detector ha un valore di confidence score\n",
    "(es: “sono 95% sicuro che questo è un cane”)\n",
    "Se scegli una soglia (es. 0.7) allora:\n",
    "Se lo score ≥ 0.7 -> accetto la predizione\n",
    "Se lo score < 0.7 -> la scarto\n",
    "Variando questa soglia ottieni situazioni diverse:\n",
    "Soglia\t    Predizioni\t            Precision\tRecall\n",
    "Alta (0.9)\tpoche ma sicure\t        ↑ alta\t    ↓ bassa\n",
    "Bassa (0.3)\ttante, anche sbagliate\t↓ bassa\t    ↑ alta\n",
    "Ogni soglia produce un punto diverso nella curva Precision–Recall\n",
    "\n",
    "Nota: la mAP è in funzione anche della soglia della IoU\n",
    "fissata lei, si calcola il valore di mAP\n",
    "\"\"\"\n",
    "def evaluate_map_from_data(predictions, targets, device=\"cuda\"):\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "\n",
    "    formatted_preds = []\n",
    "    formatted_targets = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        formatted_preds.append({\n",
    "            \"boxes\": pred[\"boxes\"].to(device),\n",
    "            \"scores\": pred[\"scores\"].to(device),\n",
    "            \"labels\": pred[\"labels\"].to(device),\n",
    "        })\n",
    "\n",
    "    for tgt in targets:\n",
    "        formatted_targets.append({\n",
    "            \"boxes\": tgt[\"boxes\"].to(device),\n",
    "            \"labels\": tgt[\"labels\"].to(device),\n",
    "        })\n",
    "\n",
    "    metric.update(formatted_preds, formatted_targets)\n",
    "    results = metric.compute()\n",
    "\n",
    "    print(\"AP Per Classe:\")\n",
    "    for i, ap in enumerate(results[\"map_per_class\"]):\n",
    "        print(f\"  Classe {i}: {float(ap):.4f}\")\n",
    "\n",
    "    print(\"RISULTATI mAP\")\n",
    "    print(f\"mAP IoU=0.50:0.95: {results['map']:.4f}\")\n",
    "    print(f\"mAP IoU=0.50:      {results['map_50']:.4f}\")\n",
    "    print(f\"mAP IoU=0.75:      {results['map_75']:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T09:26:29.447488Z",
     "iopub.status.busy": "2025-11-24T09:26:29.447205Z",
     "iopub.status.idle": "2025-11-24T09:26:29.496077Z",
     "shell.execute_reply": "2025-11-24T09:26:29.495461Z",
     "shell.execute_reply.started": "2025-11-24T09:26:29.447467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP Per Classe:\n",
      "  Classe 0: 0.7691\n",
      "  Classe 1: 0.6010\n",
      "  Classe 2: 0.7515\n",
      "RISULTATI mAP\n",
      "mAP IoU=0.50:0.95: 0.7072\n",
      "mAP IoU=0.50:      1.0000\n",
      "mAP IoU=0.75:      0.7508\n"
     ]
    }
   ],
   "source": [
    "targets = [\n",
    "    {\"boxes\": torch.tensor([[10.,10.,60.,110.],[120.,40.,200.,160.]]), \"labels\": torch.tensor([0,1])},\n",
    "    {\"boxes\": torch.tensor([[15.,18.,65.,120.],[230.,60.,350.,200.]]), \"labels\": torch.tensor([0,2])},\n",
    "    {\"boxes\": torch.tensor([[50.,40.,120.,160.]]), \"labels\": torch.tensor([0])},\n",
    "    {\"boxes\": torch.tensor([[100.,30.,180.,140.],[20.,40.,80.,100.]]), \"labels\": torch.tensor([1,0])},\n",
    "    {\"boxes\": torch.tensor([[250.,80.,400.,250.]]), \"labels\": torch.tensor([2])}\n",
    "]\n",
    "\n",
    "predictions = [\n",
    "    {\"boxes\": torch.tensor([[12.,12.,58.,108.],[118.,42.,198.,158.],[200.,200.,260.,260.]]),\n",
    "     \"scores\": torch.tensor([0.97,0.88,0.12]), \"labels\": torch.tensor([0,1,1])},\n",
    "    {\"boxes\": torch.tensor([[16.,20.,60.,115.],[220.,55.,340.,205.],[250.,50.,300.,100.]]),\n",
    "     \"scores\": torch.tensor([0.91,0.86,0.40]), \"labels\": torch.tensor([0,2,1])},\n",
    "    {\"boxes\": torch.tensor([[45.,35.,118.,162.],[200.,200.,260.,260.]]),\n",
    "     \"scores\": torch.tensor([0.95,0.22]), \"labels\": torch.tensor([0,2])},\n",
    "    {\"boxes\": torch.tensor([[22.,38.,82.,102.],[90.,20.,175.,135.],[0.,0.,30.,30.]]),\n",
    "     \"scores\": torch.tensor([0.87,0.92,0.10]), \"labels\": torch.tensor([0,1,0])},\n",
    "    {\"boxes\": torch.tensor([[250.,82.,395.,248.],[90.,90.,150.,150.]]),\n",
    "     \"scores\": torch.tensor([0.90,0.20]), \"labels\": torch.tensor([2,0])}\n",
    "]\n",
    "\n",
    "results = evaluate_map_from_data(predictions, targets, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Babysitting with randsearch DANILO\n",
    "\"\"\"\n",
    "# ==========================================\n",
    "# 1. FUNZIONE DI TRAINING (Invariata)\n",
    "# ==========================================\n",
    "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    all_losses = []\n",
    "    \n",
    "    # Barra di progresso per l'epoca\n",
    "    pbar = tqdm(loader, desc=f\"   Epoch {epoch}\", leave=False)\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            return float('inf')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        pbar.set_description(f\"   Loss: {loss_value:.4f}\")\n",
    "\n",
    "    return np.mean(all_losses)\n",
    "\n",
    "# ==========================================\n",
    "# 2. RANDOM SEARCH (LR + MOMENTUM)\n",
    "# ==========================================\n",
    "def run_random_search_tuning(start_model, train_loader, device, num_trials=10, epochs_per_trial=10):\n",
    "    print(f\"--- Inizio Random Search (LR + Momentum) ---\")\n",
    "    print(f\"Trials: {num_trials} | Epochs per Trial: {epochs_per_trial}\")\n",
    "    \n",
    "    # Salviamo i pesi iniziali per il reset\n",
    "    initial_weights = copy.deepcopy(start_model.state_dict())\n",
    "    \n",
    "    best_config = None\n",
    "    best_loss = float('inf')\n",
    "    results = []\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        # --- A. Generazione Parametri Casuali ---\n",
    "        \n",
    "        # 1. Learning Rate (Scala Logaritmica tra 1e-5 e 1e-1)\n",
    "        lr_exponent = np.random.uniform(-5, -1) \n",
    "        current_lr = 10 ** lr_exponent\n",
    "        \n",
    "        # 2. Momentum (Scala Lineare tra 0.8 e 0.99)\n",
    "        # Il momentum è solitamente alto, valori sotto 0.8 sono rari per SGD in visione\n",
    "        current_momentum = np.random.uniform(0.9, 0.99)\n",
    "        \n",
    "        print(f\"\\n=== TRIAL {trial+1}/{num_trials} | LR: {current_lr:.6f} | Momentum: {current_momentum:.4f} ===\")\n",
    "        \n",
    "        # --- B. Reset Modello ---\n",
    "        model = copy.deepcopy(start_model)\n",
    "        model.load_state_dict(initial_weights)\n",
    "        model.to(device)\n",
    "        \n",
    "        # --- C. Optimizer con i parametri random ---\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params, \n",
    "            lr=current_lr, \n",
    "            momentum=current_momentum, # <-- Parametro dinamico\n",
    "            weight_decay=0.0005\n",
    "        )\n",
    "        \n",
    "        # --- D. Training Loop ---\n",
    "        final_loss = float('inf')\n",
    "        trial_failed = False\n",
    "        \n",
    "        for epoch in range(epochs_per_trial):\n",
    "            avg_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "            \n",
    "            if avg_loss == float('inf'):\n",
    "                print(f\"   -> Trial Fallito (Loss esplosa).\")\n",
    "                trial_failed = True\n",
    "                final_loss = float('inf')\n",
    "                break\n",
    "            \n",
    "            final_loss = avg_loss\n",
    "        \n",
    "        if not trial_failed:\n",
    "            print(f\"   -> Risultato: Loss Finale = {final_loss:.6f}\")\n",
    "        \n",
    "        # Salvataggio dati\n",
    "        results.append({\n",
    "            'lr': current_lr, \n",
    "            'momentum': current_momentum, \n",
    "            'loss': final_loss\n",
    "        })\n",
    "        \n",
    "        # Aggiornamento Best\n",
    "        if final_loss < best_loss:\n",
    "            best_loss = final_loss\n",
    "            best_config = {'lr': current_lr, 'momentum': current_momentum}\n",
    "            print(f\"   >>> NUOVO RECORD! (LR={current_lr:.6f}, Mom={current_momentum:.4f})\")\n",
    "\n",
    "    # --- FINE ---\n",
    "    print(\"\\n========================================\")\n",
    "    print(\"--- CLASSIFICA FINALE (Top 3) ---\")\n",
    "    \n",
    "    # Ordiniamo per loss (dal più basso al più alto)\n",
    "    results.sort(key=lambda x: x['loss'])\n",
    "    \n",
    "    for i, res in enumerate(results[:3]):\n",
    "        print(f\"{i+1}. Loss: {res['loss']:.6f} | LR: {res['lr']:.6f} | Mom: {res['momentum']:.4f}\")\n",
    "        \n",
    "    print(f\"\\nPARAMETRI VINCITORI DA USARE:\")\n",
    "    print(f\"Optimizer: SGD\")\n",
    "    print(f\"LR:       {best_config['lr']:.6f}\")\n",
    "    print(f\"Momentum: {best_config['momentum']:.4f}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "# ==========================================\n",
    "# 3. ESECUZIONE\n",
    "# ==========================================\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "start_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "\n",
    "# Assicurati di avere 'mini_loader_train'\n",
    "if 'mini_loader_train' in locals() or 'mini_loader_train' in globals():\n",
    "    best_params = run_random_search_tuning(\n",
    "        start_model, \n",
    "        mini_loader_train, \n",
    "        device, \n",
    "        num_trials=10,       # Numero di tentativi casuali\n",
    "        epochs_per_trial=10  # Epoche per ogni tentativo\n",
    "    )\n",
    "else:\n",
    "    print(\"Devi definire 'mini_loader_train' prima di lanciare il codice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Babysitting PAOLO\n",
    "\"\"\"\n",
    "# ==========================================\n",
    "# 1. FUNZIONE DI TRAINING (Invariata)\n",
    "# ==========================================\n",
    "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    all_losses = []\n",
    "    \n",
    "    # tqdm con leave=False per non intasare la console durante i 100 step totali\n",
    "    pbar = tqdm(loader, desc=f\"    Epoch {epoch}\", leave=False)\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        # Gestione dei target come nel codice originale\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            # In caso di nan, ritorniamo infinito per scartare questo Trial\n",
    "            return float('inf')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        pbar.set_description(f\"    Loss: {loss_value:.4f}\")\n",
    "\n",
    "    # Restituisce la MEDIA della loss di tutta l'epoca\n",
    "    return np.mean(all_losses)\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOGICA RANDOM SEARCH (Modificata)\n",
    "# ==========================================\n",
    "def run_random_search(start_model, train_loader, device, num_trials=10, epochs_per_trial=10):\n",
    "    print(f\"--- Inizio Random Search per (LR, Momentum) ---\")\n",
    "    print(f\"Trials: {num_trials} | Epochs per Trial: {epochs_per_trial}\")\n",
    "    \n",
    "    # PARAMETRI PER IL CAMPIONAMENTO\n",
    "    # LR: Distribuzione Gaussiana centrata su 0.01 (10^-2) con deviazione standard logaritmica\n",
    "    LR_MEAN_EXP = -(3.2013)  # Corrisponde a 10^-2 = 0.01 (il tuo LR fissato in precedenza)\n",
    "    LR_STD_EXP = 1.0    # Deviazione standard sull'esponente logaritmico (es. per esplorare da 10^-3 a 10^-1)\n",
    "    # Momentum: Distribuzione Uniforme nell'intervallo [0.8, 0.99]\n",
    "    MOMENTUM_LOW = 0.8\n",
    "    MOMENTUM_HIGH = 0.99\n",
    "    \n",
    "    # 1. Salviamo lo stato iniziale dei pesi per resettare ogni volta\n",
    "    initial_weights = copy.deepcopy(start_model.state_dict())\n",
    "    \n",
    "    best_lr = None\n",
    "    best_momentum = None\n",
    "    best_loss = float('inf')\n",
    "    results = []\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        # A. Generiamo gli iper-parametri casuali\n",
    "        \n",
    "        # 1. Campionamento LR (Gaussiano su scala logaritmica)\n",
    "        # Genera l'esponente logaritmico usando una gaussiana\n",
    "        lr_exponent = np.random.normal(loc=LR_MEAN_EXP, scale=LR_STD_EXP) \n",
    "        current_lr = 10 ** lr_exponent\n",
    "        \n",
    "        # Clampa LR nell'intervallo ragionevole [10^-5, 10^-1] per evitare overflow\n",
    "        if current_lr < 1e-5:\n",
    "            current_lr = 1e-5\n",
    "        elif current_lr > 1e-1:\n",
    "            current_lr = 1e-1\n",
    "            \n",
    "        # 2. Campionamento Momentum (Uniforme)\n",
    "        current_momentum = np.random.uniform(MOMENTUM_LOW, MOMENTUM_HIGH)\n",
    "        \n",
    "        print(f\"\\n=== TRIAL {trial+1}/{num_trials} | Testing LR: {current_lr:.6f} | Momentum: {current_momentum:.4f} ===\")\n",
    "        \n",
    "        # B. Resettiamo il modello allo stato iniziale\n",
    "        model = copy.deepcopy(start_model)\n",
    "        model.load_state_dict(initial_weights)\n",
    "        model.to(device)\n",
    "        \n",
    "        # C. Optimizer SGD (ora include il momentum campionato)\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params, \n",
    "            lr=current_lr, \n",
    "            momentum=current_momentum, # <--- Momentum campionato\n",
    "            weight_decay=0.0005\n",
    "        )\n",
    "        \n",
    "        # D. Training Loop per questo Trial\n",
    "        final_loss = float('inf')\n",
    "        for epoch in range(epochs_per_trial):\n",
    "            avg_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "            \n",
    "            if avg_loss == float('inf'):\n",
    "                print(\"    -> Loss esplosa (NaN/Inf). Trial fallito.\")\n",
    "                final_loss = float('inf')\n",
    "                break\n",
    "            \n",
    "            final_loss = avg_loss\n",
    "            \n",
    "        print(f\"    -> Risultato Trial {trial+1}: Loss Finale = {final_loss:.6f}\")\n",
    "        \n",
    "        # Salviamo i risultati\n",
    "        results.append({'lr': current_lr, 'momentum': current_momentum, 'loss': final_loss})\n",
    "        \n",
    "        # E. Aggiorniamo il best\n",
    "        if final_loss < best_loss:\n",
    "            best_loss = final_loss\n",
    "            best_lr = current_lr\n",
    "            best_momentum = current_momentum\n",
    "            print(f\"    >>> NUOVO RECORD! LR {best_lr:.6f}, Momentum {best_momentum:.4f} è il migliore finora.\")\n",
    "\n",
    "    print(\"\\n========================================\")\n",
    "    print(\"--- FINE RANDOM SEARCH ---\")\n",
    "    print(\"========================================\")\n",
    "    \n",
    "    # Ordiniamo i risultati per loss\n",
    "    results.sort(key=lambda x: x['loss'])\n",
    "    \n",
    "    print(\"\\nClassifica Top 3 coppie (LR, Momentum):\")\n",
    "    for i, res in enumerate(results[:3]):\n",
    "        print(f\"{i+1}. LR: {res['lr']:.6f}, Momentum: {res['momentum']:.4f} -> Loss: {res['loss']:.6f}\")\n",
    "        \n",
    "    print(f\"\\nIL VINCITORE È: LR = {best_lr:.6f}, Momentum = {best_momentum:.4f}\")\n",
    "    return best_lr, best_momentum\n",
    "\n",
    "# ==========================================\n",
    "# 3. ESECUZIONE (Modificata per restituire due valori)\n",
    "# ==========================================\n",
    "\n",
    "# Configurazione Device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Usando device: {device}\")\n",
    "\n",
    "# Prepara il modello (solo architettura)\n",
    "# Abbiamo bisogno di più classi se usiamo il dataset COCO ridotto (che ha 80 classi + 1 background)\n",
    "NUM_CLASSES = 91 # Se stai usando il COCO completo (80 + 1 background) o il COCO ridotto standard. Adatta questo se il tuo dataset ridotto ha un numero di classi diverso.\n",
    "start_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=None, \n",
    "    num_classes=NUM_CLASSES # Aggiungo num_classes per chiarezza, anche se weights=None lo farebbe automaticamente.\n",
    ")\n",
    "\n",
    "# Assicurati che mini_loader_train sia definito\n",
    "if 'mini_loader_train' in locals() or 'mini_loader_train' in globals():\n",
    "    # La funzione ora ritorna la coppia (LR, Momentum)\n",
    "    best_lr, best_momentum = run_random_search(\n",
    "        start_model, \n",
    "        mini_loader_train, \n",
    "        device, \n",
    "        num_trials=10,     # Provo 10 coppie diverse\n",
    "        epochs_per_trial=10 # Addestro per 10 epoche ciascuno\n",
    "    )\n",
    "else:\n",
    "    print(\"ERRORE: Definisci 'mini_loader_train' prima di eseguire.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stessa di sopra ma plotta per ogni batch\n",
    "\"\"\"\n",
    "def train_smart_realtime(model, optimizer, train_loader, val_loader, device, num_epochs, save_path='.'):\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_sub_losses': [],\n",
    "        'val_sub_losses': []\n",
    "    }\n",
    "    \n",
    "    min_val_loss = np.inf\n",
    "    model.to(device)\n",
    "    \n",
    "    fit_time = time.time()\n",
    "    print(f\"Inizio training su {device} per {num_epochs} epoche...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        since = time.time()\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_sub_losses = {}\n",
    "        \n",
    "        # 1. Creiamo la barra di progresso (tqdm) sul train_loader\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [TRAIN]\", leave=False)\n",
    "        \n",
    "        # Usiamo pbar invece di train_loader nel ciclo\n",
    "        for images, targets in pbar:\n",
    "            \n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "\n",
    "            if not math.isfinite(loss_value):\n",
    "                print(f\"\\nLoss is {loss_value}, stopping training\")\n",
    "                sys.exit(1)\n",
    "\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulo metriche\n",
    "            running_loss += loss_value\n",
    "            for k, v in loss_dict.items():\n",
    "                if k not in running_sub_losses:\n",
    "                    running_sub_losses[k] = 0.0\n",
    "                running_sub_losses[k] += v.item()\n",
    "            \n",
    "            # 2. AGGIORNAMENTO ON-TIME: Mostriamo la loss corrente nella barra\n",
    "            # 'loss': mostra la loss dell'ultimo batch (istantanea)\n",
    "            # 'avg': mostra la media accumulata fino ad ora\n",
    "            current_avg = running_loss / (pbar.n + 1)\n",
    "            pbar.set_postfix({'batch_loss': loss_value, 'running_avg': current_avg})\n",
    "\n",
    "        # Media epocale train (calcolata alla fine della barra)\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_sub = {k: v / len(train_loader) for k, v in running_sub_losses.items()}\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        running_val_loss = 0.0\n",
    "        running_val_sub_losses = {}\n",
    "        \n",
    "        # 3. Barra di progresso anche per la validazione\n",
    "        pbar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [VAL]\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in pbar_val:\n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                loss_val_value = losses.item()\n",
    "                running_val_loss += loss_val_value\n",
    "                \n",
    "                for k, v in loss_dict.items():\n",
    "                    if k not in running_val_sub_losses:\n",
    "                        running_val_sub_losses[k] = 0.0\n",
    "                    running_val_sub_losses[k] += v.item()\n",
    "                \n",
    "                # Aggiornamento barra validazione\n",
    "                pbar_val.set_postfix({'val_loss': loss_val_value})\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        epoch_val_sub = {k: v / len(val_loader) for k, v in running_val_sub_losses.items()}\n",
    "\n",
    "        # --- SALVATAGGIO STORICO ---\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['train_sub_losses'].append(epoch_train_sub)\n",
    "        history['val_sub_losses'].append(epoch_val_sub)\n",
    "\n",
    "        # --- CHECKPOINTING ---\n",
    "        if epoch_val_loss < min_val_loss:\n",
    "            # Opzionale: stampa un messaggio quando trovi un nuovo best\n",
    "            # print(f\"  --> New Best found! ({min_val_loss:.4f} -> {epoch_val_loss:.4f})\")\n",
    "            min_val_loss = epoch_val_loss\n",
    "            torch.save(model, f'{save_path}/model_best.pt')\n",
    "\n",
    "        # --- STAMPA FINALE EPOCA (Riepilogo pulito) ---\n",
    "        time_elapsed = time.time() - since\n",
    "        \n",
    "        # Stampa pulita che non viene sovrascritta dalla barra\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Time: {time_elapsed:.0f}s | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} \"\n",
    "              f\"{'(*BEST*)' if epoch_val_loss == min_val_loss else ''}\")\n",
    "\n",
    "    total_time = time.time() - fit_time\n",
    "    print('Total training time: {:.0f}m {:.0f}s'.format(total_time // 60, total_time % 60))\n",
    "    torch.save(model, f'{save_path}/model_last.pt')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inizio Train\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "model.train()\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.004862, momentum= 0.9497,weight_decay=0.0005)\n",
    "        \n",
    "NUM_EPOCHS=5\n",
    "SAVE_DIR=\"/kaggle/working/\"\n",
    "# Usa i loader che hai già\n",
    "loader_t = mini_loader_train\n",
    "# Se non hai un val loader, usa il train loader anche per validation per ora\n",
    "loader_v = mini_loader_val\n",
    "    \n",
    "# Avvio\n",
    "history = train_smart_realtime(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=loader_t,\n",
    "        val_loader=loader_v,\n",
    "        device=device,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        save_path=SAVE_DIR\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    \"\"\"\n",
    "    Plotta le curve di loss per Train e Validation.\n",
    "    Gestisce sia la loss totale che le sotto-loss (se presenti).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Recupero Dati Principali\n",
    "    train_loss = history.get('train_loss', [])\n",
    "    val_loss = history.get('val_loss', [])\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    # --- GRAFICO 1: LOSS TOTALE ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_loss, 'b-', label='Training Loss', linewidth=2)\n",
    "    \n",
    "    if val_loss:\n",
    "        plt.plot(epochs, val_loss, 'r--', label='Validation Loss', linewidth=2)\n",
    "        \n",
    "        # Evidenzia il punto minimo della validation loss\n",
    "        min_val = min(val_loss)\n",
    "        min_epoch = val_loss.index(min_val) + 1\n",
    "        plt.scatter(min_epoch, min_val, s=100, c='green', marker='o', label=f'Best Model (Ep {min_epoch})')\n",
    "        plt.annotate(f'{min_val:.4f}', (min_epoch, min_val), textcoords=\"offset points\", xytext=(0,10), ha='center', color='green')\n",
    "\n",
    "    plt.title('Andamento Loss Totale (Faster R-CNN)', fontsize=16)\n",
    "    plt.xlabel('Epoche', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # --- GRAFICO 2: SOTTO-LOSS (CLASSIFIER, BOX, RPN...) ---\n",
    "    # Questo parte solo se la history ha salvato i dettagli (train_sub_losses)\n",
    "    if 'train_sub_losses' in history and len(history['train_sub_losses']) > 0:\n",
    "        \n",
    "        # Recuperiamo le chiavi delle sotto-loss dal primo elemento\n",
    "        keys = history['train_sub_losses'][0].keys()\n",
    "        num_plots = len(keys)\n",
    "        \n",
    "        # Creiamo una griglia di grafici\n",
    "        rows = math.ceil(num_plots / 2)\n",
    "        fig, axes = plt.subplots(rows, 2, figsize=(15, 5 * rows))\n",
    "        fig.suptitle('Dettaglio Componenti della Loss', fontsize=16)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, key in enumerate(keys):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Estrai i dati per questa specifica loss (es. loss_classifier)\n",
    "            sub_train = [h[key] for h in history['train_sub_losses']]\n",
    "            \n",
    "            ax.plot(epochs, sub_train, label=f'Train {key}', color='tab:blue')\n",
    "            \n",
    "            # Se abbiamo anche i dettagli della validation\n",
    "            if 'val_sub_losses' in history and len(history['val_sub_losses']) > 0:\n",
    "                sub_val = [h[key] for h in history['val_sub_losses']]\n",
    "                ax.plot(epochs, sub_val, label=f'Val {key}', color='tab:orange', linestyle='--')\n",
    "            \n",
    "            ax.set_title(key)\n",
    "            ax.set_xlabel('Epoche')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.5)\n",
    "            \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "# --- ESEMPIO DI UTILIZZO ---\n",
    "# Assicurati di aver eseguito il training prima di lanciare questa riga\n",
    "if 'history' in locals():\n",
    "    plot_losses(history)\n",
    "else:\n",
    "    print(\"Variabile 'history' non trovata. Esegui prima il training!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8758877,
     "sourceId": 13763433,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8758945,
     "sourceId": 13763536,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
